# MetaDrive PPOè®­ç»ƒç³»ç»Ÿæ–‡ä»¶è¯´æ˜

## ğŸ“– æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«å®Œæ•´çš„MetaDriveå¼ºåŒ–å­¦ä¹ PPOè®­ç»ƒç³»ç»Ÿï¼Œæ”¯æŒéšæœºåœºæ™¯ç”Ÿæˆã€å¯è§†åŒ–è®­ç»ƒç›‘æ§ã€æ¨¡å‹æµ‹è¯•ç­‰åŠŸèƒ½ã€‚

---

## ğŸ—‚ï¸ æ ¸å¿ƒæ–‡ä»¶ç»“æ„

### 1. **random_scenario_generator.py** ğŸ²
**åŠŸèƒ½**: éšæœºåœºæ™¯ç”Ÿæˆå™¨  
**æ ¸å¿ƒç±»**: `RandomScenarioGenerator`, `MetaDriveRandomEnv`

#### ä¸»è¦åŠŸèƒ½:
- **å¤šç»´åº¦éšæœºåŒ–**:
  - ğŸ—ºï¸ åœ°å›¾ç»“æ„: é•¿åº¦ã€å¤æ‚åº¦ã€è½¦é“æ•°ã€å¼¯é“å¯†åº¦
  - ğŸš— äº¤é€šç¯å¢ƒ: è½¦æµå¯†åº¦ã€äº‹æ•…æ¦‚ç‡ã€äº¤é€šæ¨¡å¼
  - ğŸŒ¤ï¸ ç¯å¢ƒæ¡ä»¶: å¤©æ°”ã€å…‰ç…§ã€åœ°å½¢
  - ğŸ¯ ä»»åŠ¡éš¾åº¦: èµ·å§‹ä½ç½®ã€ç›®æ ‡è·ç¦»ã€æ—¶é—´é™åˆ¶

- **ä¸‰ç§è®­ç»ƒæ¨¡å¼**:
  - `"random"`: å®Œå…¨éšæœºåœºæ™¯ï¼Œé€‚åˆåŸºç¡€è®­ç»ƒ
  - `"curriculum"`: è¯¾ç¨‹å­¦ä¹ ï¼Œéš¾åº¦é€’å¢
  - `"safe"`: å®‰å…¨é©¾é©¶åœºæ™¯ï¼Œå¼ºè°ƒå®‰å…¨æ€§

#### æ ¸å¿ƒæ–¹æ³•:
```python
# ç”ŸæˆåŸºç¡€éšæœºåœºæ™¯
generate_basic_scenarios(num_scenarios=100)

# ç”Ÿæˆè¯¾ç¨‹å­¦ä¹ åœºæ™¯
generate_curriculum_scenarios(max_difficulty=5)

# ç”Ÿæˆå®‰å…¨é©¾é©¶åœºæ™¯
generate_safe_scenarios(accident_prob_range=(0.1, 0.3))
```

#### ä½¿ç”¨ç¤ºä¾‹:
```python
from random_scenario_generator import RandomScenarioGenerator, MetaDriveRandomEnv

# åˆ›å»ºç”Ÿæˆå™¨
generator = RandomScenarioGenerator(seed=42)

# åˆ›å»ºè®­ç»ƒç¯å¢ƒ
env = MetaDriveRandomEnv(
    generator=generator,
    scenario_type="random",
    base_config={"use_render": True}
)
```

---

### 2. **sb3_ppo_integration.py** ğŸ”—
**åŠŸèƒ½**: Stable Baselines3é›†æˆé€‚é…å™¨  
**æ ¸å¿ƒç±»**: `SB3TrajectoryReplayWrapper`

#### ä¸»è¦åŠŸèƒ½:
- **SB3å…¼å®¹æ€§**: ç¡®ä¿ä¸Stable Baselines3çš„gym.Envæ¥å£å®Œå…¨å…¼å®¹
- **ç¯å¢ƒåŒ…è£…**: å¤„ç†observation_spaceå’Œaction_space
- **å¥–åŠ±è®¾è®¡**: æä¾›å¯é…ç½®çš„å¥–åŠ±å‡½æ•°
- **æ ¼å¼ç»Ÿä¸€**: ç»Ÿä¸€stepè¿”å›æ ¼å¼ï¼ˆ5å…ƒç»„ï¼‰

#### å¥–åŠ±æœºåˆ¶:
```python
reward_config = {
    "success_reward": 10.0,        # æˆåŠŸå®Œæˆå¥–åŠ±
    "crash_penalty": -5.0,         # ç¢°æ’æƒ©ç½š
    "out_of_road_penalty": -3.0,   # å‡ºè·¯æƒ©ç½š
    "speed_reward_factor": 0.1,    # é€Ÿåº¦å¥–åŠ±ç³»æ•°
    "steering_penalty_factor": 0.02 # è½¬å‘æƒ©ç½šç³»æ•°
}
```

#### æ ¸å¿ƒæ–¹æ³•:
```python
# è®­ç»ƒPPOæ¨¡å‹
train_sb3_ppo(
    csv_path="trajectory.csv",
    total_timesteps=50000,
    learning_rate=3e-4
)

# åŠ è½½å’Œæµ‹è¯•æ¨¡å‹
load_and_test_sb3_model(
    model_path="trained_model.zip",
    csv_path="test_trajectory.csv"
)
```

---

### 3. **visual_training_monitor.py** ğŸ“Š
**åŠŸèƒ½**: å¯è§†åŒ–è®­ç»ƒç›‘æ§ç³»ç»Ÿ  
**æ ¸å¿ƒç±»**: `VisualTrainingEnv`, `VisualTrainingCallback`

#### ä¸»è¦åŠŸèƒ½:
- **å®æ—¶æ¸²æŸ“**: MetaDriveçª—å£æ˜¾ç¤ºè®­ç»ƒåœºæ™¯
- **è®­ç»ƒç»Ÿè®¡**: æ”¶é›†episodeå¥–åŠ±ã€é•¿åº¦ã€åŠ¨ä½œã€é€Ÿåº¦å†å²
- **å®æ—¶å›¾è¡¨**: Matplotlibæ˜¾ç¤ºè®­ç»ƒæ›²çº¿
- **HUDæ˜¾ç¤º**: åœ¨MetaDriveçª—å£æ˜¾ç¤ºè®­ç»ƒä¿¡æ¯

#### å¯è§†åŒ–å†…å®¹:
1. **Episode Rewards**: å¥–åŠ±å€¼å˜åŒ–è¶‹åŠ¿
2. **Episode Lengths**: æ¯ä¸ªepisodeæŒç»­æ­¥æ•°
3. **Action Distribution**: è½¬å‘vsæ²¹é—¨çš„åŠ¨ä½œåˆ†å¸ƒçƒ­å›¾
4. **Speed Distribution**: è½¦è¾†é€Ÿåº¦åˆ†å¸ƒç›´æ–¹å›¾

#### è®­ç»ƒç¯å¢ƒç‰¹æ€§:
```python
class VisualTrainingEnv(MetaDriveRandomEnv):
    def render_training_info(self):
        """åœ¨MetaDrive HUDæ˜¾ç¤ºè®­ç»ƒä¿¡æ¯"""
        training_text = {
            "Episode": f"{self.episode_count}",
            "Steps": f"{self.step_count}",
            "Reward": f"{self.current_episode_reward:.2f}",
            "Speed": f"{self.agent.speed:.1f} m/s"
        }
```

#### å…³é”®é…ç½®:
```python
visual_config = {
    "render_mode": "human",
    "window_size": (1200, 800),
    "show_sensors": True,
    "show_trajectory": True,
    "record_video": False
}
```

---

### 4. **train_ppo_with_random_scenarios.py** ğŸ¯
**åŠŸèƒ½**: ä¸»PPOè®­ç»ƒè„šæœ¬  
**æ ¸å¿ƒç±»**: `MetaDriveRandomWrapper`

#### ä¸»è¦åŠŸèƒ½:
- **ç¯å¢ƒåŒ…è£…**: å°†MetaDriveç¯å¢ƒåŒ…è£…ä¸ºSB3å…¼å®¹æ ¼å¼
- **å¹¶è¡Œè®­ç»ƒ**: æ”¯æŒå¤šè¿›ç¨‹è®­ç»ƒåŠ é€Ÿ
- **è¶…å‚æ•°ç®¡ç†**: å®Œæ•´çš„è®­ç»ƒå‚æ•°é…ç½®
- **æ—¥å¿—è®°å½•**: TensorBoardå’ŒCSVæ—¥å¿—

#### è®­ç»ƒæµç¨‹:
1. **ç¯å¢ƒåˆ›å»º**: åˆ›å»ºéšæœºåœºæ™¯ç¯å¢ƒ
2. **æ¨¡å‹åˆå§‹åŒ–**: é…ç½®PPOæ¨¡å‹å‚æ•°
3. **è®­ç»ƒæ‰§è¡Œ**: å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
4. **æ¨¡å‹ä¿å­˜**: ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹

#### æ ¸å¿ƒé…ç½®:
```python
training_config = {
    "total_timesteps": 100000,
    "learning_rate": 3e-4,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 10,
    "gamma": 0.99,
    "gae_lambda": 0.95
}
```

#### å‘½ä»¤è¡Œä½¿ç”¨:
```bash
python train_ppo_with_random_scenarios.py \
    --scenario-type random \
    --total-timesteps 50000 \
    --learning-rate 3e-4 \
    --log-dir ./logs/ppo_training
```

---

### 5. **start_visual_training.py** ğŸ¬
**åŠŸèƒ½**: å¯è§†åŒ–è®­ç»ƒå¯åŠ¨å™¨  
**ç±»å‹**: å‘½ä»¤è¡Œå¯åŠ¨è„šæœ¬

#### ä¸»è¦åŠŸèƒ½:
- **ä¸€é”®å¯åŠ¨**: ç®€åŒ–çš„å¯è§†åŒ–è®­ç»ƒå¯åŠ¨
- **å‚æ•°é…ç½®**: å®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
- **ç”¨æˆ·å‹å¥½**: æ¸…æ™°çš„ä½¿ç”¨è¯´æ˜å’Œæç¤º

#### æ”¯æŒå‚æ•°:
```bash
# åŸºç¡€é…ç½®
--scenario-type {random,curriculum,safe}
--total-timesteps 50000
--seed 42

# è®­ç»ƒå‚æ•°
--learning-rate 3e-4
--n-steps 1024
--batch-size 64

# å¯è§†åŒ–é…ç½®
--window-size 1200 800
--log-freq 500
--plot-freq 2000

# è¾“å‡ºé…ç½®
--log-dir ./logs/ppo_visual
--device {auto,cpu,cuda}
```

#### ä½¿ç”¨ç¤ºä¾‹:
```bash
# åŸºç¡€è®­ç»ƒ
python start_visual_training.py

# è‡ªå®šä¹‰é…ç½®
python start_visual_training.py \
    --scenario-type curriculum \
    --total-timesteps 100000 \
    --learning-rate 1e-3 \
    --window-size 1600 900
```

---

### 6. **trained_model_test.py** ğŸ§ª
**åŠŸèƒ½**: è®­ç»ƒæ¨¡å‹æµ‹è¯•å·¥å…·  
**ç±»å‹**: æ¨¡å‹è¯„ä¼°è„šæœ¬

#### ä¸»è¦åŠŸèƒ½:
- **æ¨¡å‹åŠ è½½**: åŠ è½½è®­ç»ƒå¥½çš„PPOæ¨¡å‹
- **æ€§èƒ½è¯„ä¼°**: åœ¨æ–°åœºæ™¯ä¸­æµ‹è¯•æ¨¡å‹æ€§èƒ½
- **ç»Ÿè®¡åˆ†æ**: è¯¦ç»†çš„æµ‹è¯•ç»“æœç»Ÿè®¡
- **æ¸²æŸ“é€‰æ‹©**: æ”¯æŒæœ‰æ¸²æŸ“/æ— æ¸²æŸ“æµ‹è¯•

#### æµ‹è¯•ç¯å¢ƒé…ç½®:
```python
test_config = {
    "use_render": False,  # å¯é€‰æ‹©å¼€å¯æ¸²æŸ“
    "horizon": 1000,      # æ›´é•¿çš„æµ‹è¯•æ—¶é—´
    "traffic_density": 0.2,
    "num_scenarios": 50,
    "start_seed": 123     # ä¸åŒç§å­æµ‹è¯•æ³›åŒ–èƒ½åŠ›
}
```

#### è¯„ä¼°æŒ‡æ ‡:
- **å¹³å‡å¥–åŠ±**: å¤šä¸ªepisodeçš„å¹³å‡ç´¯ç§¯å¥–åŠ±
- **å¹³å‡é•¿åº¦**: episodeå¹³å‡æŒç»­æ­¥æ•°
- **æœ€ä½³æ€§èƒ½**: æœ€é«˜å¥–åŠ±å’Œæœ€é•¿episode
- **æˆåŠŸç‡**: å®Œæˆä»»åŠ¡çš„episodeæ¯”ä¾‹

#### ä½¿ç”¨ç¤ºä¾‹:
```bash
# æ— æ¸²æŸ“æµ‹è¯•
python trained_model_test.py \
    --model-path ./logs/ppo_training/final_model.zip \
    --episodes 10

# æœ‰æ¸²æŸ“æµ‹è¯•
python trained_model_test.py \
    --model-path ./logs/ppo_training/final_model.zip \
    --episodes 5 \
    --render
```

---

## ğŸ”§ ç³»ç»Ÿä¾èµ–

### PythonåŒ…ä¾èµ–:
```bash
# æ ¸å¿ƒä¾èµ–
pip install stable-baselines3[extra]
pip install gymnasium
pip install numpy
pip install matplotlib

# MetaDriveç›¸å…³
pip install metadrive-simulator

# å¯é€‰ä¾èµ–
pip install tensorboard  # è®­ç»ƒç›‘æ§
pip install tqdm rich    # è¿›åº¦æ¡æ˜¾ç¤º
```

### ç³»ç»Ÿä¾èµ–:
```bash
# Ubuntu/Debian
sudo apt-get install -y libxcb-xinerama0 libxcb-xfixes0 \
    libxcb-randr0 libxcb-icccm4 libxcb-image0 \
    libxcb-keysyms1 libxcb-render-util0 \
    libxcb-shape0 qtbase5-dev qt5-qmake
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€è®­ç»ƒ (æ— å¤´æ¨¡å¼)
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/metadrive/a_scen_env

# æ¿€æ´»condaç¯å¢ƒ
conda activate metadrive

# å¼€å§‹è®­ç»ƒ
python train_ppo_with_random_scenarios.py \
    --scenario-type random \
    --total-timesteps 10000
```

### 2. å¯è§†åŒ–è®­ç»ƒ
```bash
# å¯åŠ¨å¯è§†åŒ–è®­ç»ƒ
python start_visual_training.py \
    --total-timesteps 50000 \
    --device cpu
```

### 3. æ¨¡å‹æµ‹è¯•
```bash
# æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
python trained_model_test.py \
    --model-path ./logs/ppo_training/final_model.zip \
    --episodes 5 \
    --render
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§

### TensorBoardå¯è§†åŒ–:
```bash
# å¯åŠ¨TensorBoard
tensorboard --logdir ./logs

# åœ¨æµè§ˆå™¨è®¿é—®
http://localhost:6006
```

### æ—¥å¿—æ–‡ä»¶:
- `progress.csv`: è®­ç»ƒè¿›åº¦æ•°æ®
- `events.out.tfevents.*`: TensorBoardäº‹ä»¶æ–‡ä»¶
- `final_model.zip`: æœ€ç»ˆè®­ç»ƒæ¨¡å‹

---

## ğŸ›ï¸ é«˜çº§é…ç½®

### 1. è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°
```python
custom_reward_config = {
    "success_reward": 20.0,
    "crash_penalty": -10.0,
    "speed_reward_factor": 0.2,
    "efficiency_bonus": 5.0
}
```

### 2. è¯¾ç¨‹å­¦ä¹ é…ç½®
```python
curriculum_config = {
    "difficulty_progression": "linear",
    "max_difficulty": 5,
    "episodes_per_level": 100
}
```

### 3. å¹¶è¡Œè®­ç»ƒé…ç½®
```python
parallel_config = {
    "n_envs": 4,           # å¹¶è¡Œç¯å¢ƒæ•°é‡
    "vec_env_cls": SubprocVecEnv,  # å¤šè¿›ç¨‹ç¯å¢ƒ
    "env_kwargs": {}       # ç¯å¢ƒå‚æ•°
}
```

---

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ:

1. **Qtå¹³å°æ’ä»¶é”™è¯¯**:
   ```bash
   export QT_QPA_PLATFORM=offscreen
   ```

2. **CUDAå†…å­˜ä¸è¶³**:
   ```bash
   python script.py --device cpu
   ```

3. **ä¾èµ–åŒ…ç¼ºå¤±**:
   ```bash
   pip install stable-baselines3[extra]
   pip install tqdm rich
   ```

4. **MetaDriveæ¸²æŸ“é—®é¢˜**:
   ```python
   # ä½¿ç”¨æ— å¤´æ¨¡å¼
   config = {"use_render": False}
   ```

---

## ğŸ“ å¼€å‘è¯´æ˜

### æ‰©å±•æŒ‡å—:
1. **æ·»åŠ æ–°åœºæ™¯ç±»å‹**: åœ¨`RandomScenarioGenerator`ä¸­æ·»åŠ æ–°çš„ç”Ÿæˆæ–¹æ³•
2. **è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°**: ä¿®æ”¹`SB3TrajectoryReplayWrapper`ä¸­çš„å¥–åŠ±è®¡ç®—
3. **æ–°å¢å¯è§†åŒ–å†…å®¹**: åœ¨`VisualTrainingCallback`ä¸­æ·»åŠ æ–°çš„å›¾è¡¨
4. **ä¼˜åŒ–è®­ç»ƒå‚æ•°**: è°ƒæ•´PPOè¶…å‚æ•°ä»¥è·å¾—æ›´å¥½æ€§èƒ½

### ä»£ç ç»“æ„:
```
a_scen_env/
â”œâ”€â”€ random_scenario_generator.py    # åœºæ™¯ç”Ÿæˆæ ¸å¿ƒ
â”œâ”€â”€ sb3_ppo_integration.py         # SB3é›†æˆé€‚é…
â”œâ”€â”€ visual_training_monitor.py     # å¯è§†åŒ–ç›‘æ§
â”œâ”€â”€ train_ppo_with_random_scenarios.py  # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ start_visual_training.py       # å¯åŠ¨å™¨
â”œâ”€â”€ trained_model_test.py          # æ¨¡å‹æµ‹è¯•
â””â”€â”€ logs/                          # è®­ç»ƒæ—¥å¿—ç›®å½•
```

---

## ğŸ‰ æ€»ç»“

è¿™å¥—PPOè®­ç»ƒç³»ç»Ÿæä¾›äº†å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµç¨‹ï¼Œä»éšæœºåœºæ™¯ç”Ÿæˆåˆ°æ¨¡å‹è®­ç»ƒã€å¯è§†åŒ–ç›‘æ§ï¼Œå†åˆ°æ¨¡å‹æµ‹è¯•è¯„ä¼°ã€‚ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•å’Œå®šåˆ¶ï¼Œé€‚åˆç ”ç©¶å’Œå®é™…åº”ç”¨ã€‚

**ä¸»è¦ç‰¹è‰²**:
- ğŸ² ä¸°å¯Œçš„éšæœºåœºæ™¯ç”Ÿæˆ
- ğŸ“Š å®æ—¶å¯è§†åŒ–è®­ç»ƒç›‘æ§  
- ğŸ”§ å®Œå–„çš„SB3é›†æˆ
- ğŸ§ª å…¨é¢çš„æ¨¡å‹æµ‹è¯•è¯„ä¼°
- ğŸš€ ç®€å•æ˜“ç”¨çš„å¯åŠ¨æ–¹å¼ 