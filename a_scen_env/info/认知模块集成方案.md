# 认知模块集成方案：轨迹重放环境主车控制

## 📋 概述

本文档详细分析了如何将 `cognitive_wrappers.py` 中的认知模块集成到 `trajectory_replay.py` 中的主车控制系统，实现基于认知建模的智能驾驶行为。

## 🔍 现状分析

### 1. 认知模块结构 (`cognitive_wrappers.py`)

认知模块包含三个核心Wrapper：

- **PerceptionWrapper (S→O)**: 感知误差模拟 + 卡尔曼滤波
- **CognitiveBiasWrapper (O→A)**: 基于TTA的决策偏差
- **DelayWrapper (A→S)**: 执行延迟模拟

### 2. 轨迹重放环境控制结构 (`trajectory_replay.py`)

当前控制流程：
```
环境观测 → 控制模式管理器 → 动作执行 → 环境更新
```

控制模式管理器支持：
- PPO专家接管
- 键盘手动控制  
- 轨迹重放模式

## 🎯 集成目标

将认知模块无缝集成到主车控制流程中，实现：

1. **感知层面**: 模拟人类感知误差，通过卡尔曼滤波优化观测（仅PPO模式）
2. **决策层面**: 基于TTA的认知偏差，影响PPO专家的决策行为
3. **执行层面**: 模拟人类反应延迟，使动作更真实（仅PPO模式）

**注意**: 感知和执行延迟模块仅在PPO专家模式下生效，手动控制和轨迹回放模式不受影响

## 🏗️ 集成架构设计

### 整体架构图

```
原始环境观测 → PerceptionWrapper → 认知偏差计算 → 控制模式管理器 → DelayWrapper → 动作执行
     ↓              ↓                    ↓              ↓              ↓
   真实状态     感知误差+滤波         TTA偏差调整      PPO/手动/轨迹     执行延迟
```

**说明**: PerceptionWrapper和DelayWrapper仅在PPO专家模式下生效

### 详细集成流程

```
1. 环境观测 (S)
   ↓
2. PerceptionWrapper: 添加感知误差 + 卡尔曼滤波 (仅PPO模式)
   ↓  
3. 认知偏差计算: 基于TTA调整观测/奖励
   ↓
4. 控制模式管理器: 选择控制策略 (PPO/手动/轨迹)
   ↓
5. DelayWrapper: 应用执行延迟 (仅PPO模式)
   ↓
6. 动作执行 (A)
   ↓
7. 环境状态更新 (S)
```

**注意**: 感知误差和执行延迟仅在PPO专家模式下应用，手动控制和轨迹回放保持原始行为

## 🔧 具体实现方案

### 方案1: 环境级包装 (推荐)

将认知模块作为环境的外层包装，在 `TrajectoryReplayEnv` 外部应用：

```python
# 创建基础环境
base_env = TrajectoryReplayEnv(traj_data, config)

# 应用认知模块包装
cognitive_env = create_cognitive_env(
    base_env=base_env,
    config={
        'perception_sigma': 0.5,
        'enable_kalman': True,
        'Inverse_TTA_Coef': 1.0,
        'tta_threshold': 2.0,
        'delay_steps': 2,
        'enable_smoothing': True
    },
    enable_perception=True,
    enable_cognitive_bias=True,
    enable_delay=True
)
```

**优点**: 
- 模块化程度高，易于配置和调试
- 不影响原有环境代码
- 支持灵活的组合使用

**缺点**: 
- 需要修改环境创建代码
- 认知模块无法直接访问环境内部状态

### 方案2: 控制管理器集成

将认知模块集成到 `ControlModeManager` 中：

```python
class CognitiveControlModeManager(ControlModeManager):
    def __init__(self, engine, main_vehicle_trajectory=None, cognitive_config=None):
        super().__init__(engine, main_vehicle_trajectory)
        
        # 初始化认知模块
        self.perception_wrapper = None
        self.cognitive_bias_wrapper = None
        self.delay_wrapper = None
        
        if cognitive_config:
            self._initialize_cognitive_modules(cognitive_config)
    
    def _initialize_cognitive_modules(self, config):
        # 初始化感知模块
        if config.get('enable_perception', False):
            self.perception_wrapper = PerceptionWrapper(
                env=None,  # 需要特殊处理
                sigma=config.get('perception_sigma', 0.5),
                enable_kalman=config.get('enable_kalman', True)
            )
        
        # 初始化认知偏差模块
        if config.get('enable_cognitive_bias', False):
            self.cognitive_bias_wrapper = CognitiveBiasWrapper(
                env=None,  # 需要特殊处理
                Inverse_TTA_Coef=config.get('Inverse_TTA_Coef', 1.0),
                tta_threshold=config.get('tta_threshold', 2.0)
            )
        
        # 初始化延迟模块
        if config.get('enable_delay', False):
            self.delay_wrapper = DelayWrapper(
                env=None,  # 需要特殊处理
                delay_steps=config.get('delay_steps', 2),
                enable_smoothing=config.get('enable_smoothing', True)
            )
    
    def get_action(self, default_action):
        # 获取原始动作
        action, action_info = super().get_action(default_action)
        
        # 应用认知模块处理
        if self.delay_wrapper:
            action = self.delay_wrapper.action(action)
        
        return action, action_info
```

**优点**: 
- 认知模块直接集成到控制流程中
- 可以访问环境内部状态
- 控制逻辑集中管理

**缺点**: 
- 需要重构现有控制管理器
- 代码耦合度增加

### 方案3: 混合集成 (推荐)

结合方案1和方案2的优点，在环境内部集成认知模块：

```python
class TrajectoryReplayEnv(MetaDriveEnv):
    def __init__(self, trajectory_dict, config=None):
        # ... 现有初始化代码 ...
        
        # 认知模块配置
        self.cognitive_config = config.get('cognitive_config', {})
        self.enable_cognitive_modules = config.get('enable_cognitive_modules', False)
        
        # 初始化认知模块
        if self.enable_cognitive_modules:
            self._initialize_cognitive_modules()
    
    def _initialize_cognitive_modules(self):
        """初始化认知模块"""
        # 感知模块
        if self.cognitive_config.get('enable_perception', False):
            self.perception_wrapper = PerceptionWrapper(
                env=None,  # 特殊处理
                sigma=self.cognitive_config.get('perception_sigma', 0.5),
                enable_kalman=self.cognitive_config.get('enable_kalman', True)
            )
        
        # 认知偏差模块
        if self.cognitive_config.get('enable_cognitive_bias', False):
            self.cognitive_bias_wrapper = CognitiveBiasWrapper(
                env=None,  # 特殊处理
                Inverse_TTA_Coef=self.cognitive_config.get('Inverse_TTA_Coef', 1.0),
                tta_threshold=self.cognitive_config.get('tta_threshold', 2.0)
            )
        
        # 延迟模块
        if self.cognitive_config.get('enable_delay', False):
            self.delay_wrapper = DelayWrapper(
                env=None,  # 特殊处理
                delay_steps=self.cognitive_config.get('delay_steps', 2),
                enable_smoothing=self.cognitive_config.get('enable_smoothing', True)
            )
    
    def step(self, action):
        # 应用执行延迟（仅PPO模式）
        if (hasattr(self, 'delay_wrapper') and self.delay_wrapper and 
            self.control_manager.expert_mode and 
            getattr(self.agent, 'expert_takeover', False)):
            action = self.delay_wrapper.action(action)
        
        # ... 现有step逻辑 ...
        
        # 应用认知偏差到奖励
        if hasattr(self, 'cognitive_bias_wrapper') and self.cognitive_bias_wrapper:
            reward = self.cognitive_bias_wrapper.reward(reward)
        
        return obs, reward, done, info
    
    def _get_observation(self):
        """获取观测并应用感知误差（仅PPO模式）"""
        obs = super()._get_observation()
        
        if (hasattr(self, 'perception_wrapper') and self.perception_wrapper and
            self.control_manager.expert_mode and 
            getattr(self.agent, 'expert_takeover', False)):
            # 获取主车状态用于感知误差计算
            ego_state = np.array([
                self.agent.position[0], 
                self.agent.position[1]
            ])
            obs = self.perception_wrapper.observation_with_ego_state(obs, ego_state)
        
        return obs
```

## 📊 配置参数设计

### 认知模块配置结构

```python
COGNITIVE_CONFIG = {
    # 感知模块配置
    'perception': {
        'enable': True,
        'sigma': 0.5,                    # 感知噪声标准差
        'enable_kalman': True,           # 启用卡尔曼滤波
        'process_noise': 0.1,            # 过程噪声
        'dt': 0.02                       # 时间步长
    },
    
    # 认知偏差模块配置
    'cognitive_bias': {
        'enable': True,
        'Inverse_TTA_Coef': 1.0,        # TTA偏差系数
        'tta_threshold': 2.0,            # TTA阈值
        'adaptive_bias': True,           # 自适应偏差
        'strict_mode': False             # 严格模式
    },
    
    # 执行延迟模块配置
    'delay': {
        'enable': True,
        'delay_steps': 2,                # 延迟步数
        'enable_smoothing': True,        # 启用平滑
        'smoothing_factor': 0.3          # 平滑因子
    }
}
```

### 环境配置示例

```python
env = TrajectoryReplayEnv(
    traj_data, 
    config={
        'use_render': True,
        'manual_control': True,
        'enable_background_vehicles': True,
        
        # 认知模块配置
        'enable_cognitive_modules': True,
        'cognitive_config': COGNITIVE_CONFIG
    }
)
```

## 🔄 集成步骤

### 步骤1: 修改认知模块接口

需要修改 `cognitive_wrappers.py` 中的Wrapper类，使其能够：

1. **独立工作**: 不依赖完整的gym环境
2. **状态访问**: 能够获取环境状态信息
3. **配置灵活**: 支持运行时配置调整

### 步骤2: 集成到轨迹重放环境

1. 在 `TrajectoryReplayEnv.__init__()` 中添加认知模块初始化
2. 在 `step()` 方法中集成认知模块处理
3. 在 `reset()` 方法中重置认知模块状态
4. 添加认知模块配置参数

### 步骤3: 修改控制管理器

1. 在 `ControlModeManager` 中添加认知模块支持
2. 修改动作获取流程，集成认知处理
3. 添加认知模块状态显示

### 步骤4: 测试和验证

1. 单元测试：测试各个认知模块功能
2. 集成测试：测试完整控制流程
3. 性能测试：验证认知模块对性能的影响

## ⚠️ 注意事项

### 1. 性能考虑

- 卡尔曼滤波计算复杂度：O(n³)，n为状态维度
- 认知偏差计算：需要实时计算TTA值
- 执行延迟：增加内存使用和计算开销

### 2. 兼容性问题

- 确保认知模块不影响现有功能
- 保持向后兼容性
- 处理认知模块初始化失败的情况

### 3. 调试和监控

- 添加认知模块状态日志
- 提供认知模块参数实时调整接口
- 监控认知模块对控制性能的影响

## 🚀 扩展功能

### 1. 动态参数调整

支持运行时调整认知模块参数：

```python
# 实时调整感知噪声
env.perception_wrapper.sigma = 0.8

# 实时调整TTA偏差系数
env.cognitive_bias_wrapper.Inverse_TTA_Coef = 1.5

# 实时调整延迟步数
env.delay_wrapper.delay_steps = 3
```

### 2. 认知状态可视化

在渲染界面显示认知模块状态：

```python
def render(self, *args, **kwargs):
    render_text = kwargs.get("text", {})
    
    # 添加认知模块状态信息
    if hasattr(self, 'perception_wrapper'):
        render_text.update({
            "Perception Noise": f"{self.perception_wrapper.sigma:.3f}",
            "Kalman Status": "ON" if self.perception_wrapper.enable_kalman else "OFF"
        })
    
    if hasattr(self, 'cognitive_bias_wrapper'):
        render_text.update({
            "TTA Bias": f"{self.cognitive_bias_wrapper.Inverse_TTA_Coef:.2f}",
            "Current TTA": f"{self._get_current_tta():.2f}"
        })
    
    if hasattr(self, 'delay_wrapper'):
        render_text.update({
            "Action Delay": f"{self.delay_wrapper.delay_steps} steps",
            "Smoothing": "ON" if self.delay_wrapper.enable_smoothing else "OFF"
        })
    
    kwargs["text"] = render_text
    return super().render(*args, **kwargs)
```

### 3. 认知模块组合配置

支持灵活的认知模块组合：

```python
# 只启用感知模块
env = TrajectoryReplayEnv(traj_data, config={
    'enable_cognitive_modules': True,
    'cognitive_config': {
        'perception': {'enable': True, 'sigma': 0.5},
        'cognitive_bias': {'enable': False},
        'delay': {'enable': False}
    }
})

# 只启用认知偏差模块
env = TrajectoryReplayEnv(traj_data, config={
    'enable_cognitive_modules': True,
    'cognitive_config': {
        'perception': {'enable': False},
        'cognitive_bias': {'enable': True, 'Inverse_TTA_Coef': 1.0},
        'delay': {'enable': False}
    }
})
```

## 📝 总结

通过将认知模块集成到轨迹重放环境的主车控制中，可以实现：

1. **更真实的驾驶行为**: 模拟人类感知误差、认知偏差和执行延迟
2. **灵活的控制策略**: 支持多种认知模块组合配置
3. **可扩展的架构**: 易于添加新的认知特性

推荐使用**方案3（混合集成）**，既保持了模块化设计，又能充分利用环境内部状态信息，实现最佳的集成效果。 