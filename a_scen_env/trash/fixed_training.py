#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆPPOè®­ç»ƒè„šæœ¬ - è§£å†³æ‰€æœ‰å·²çŸ¥é—®é¢˜

é›†æˆä¿®å¤ï¼š
1. æ¸²æŸ“é”™è¯¯ä¿®å¤
2. åŠ¨ä½œå¹³æ»‘
3. Episodeç®¡ç†ï¼ˆé˜²æ­¢å¡æ­»ï¼‰
4. ç¨³å®šçš„PPOé…ç½®
5. è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯

ä½¿ç”¨æ–¹æ³•ï¼š
    python fixed_training.py --total-timesteps 50000
"""

import argparse
import sys
import os
import numpy as np
import time

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥ä¾èµ–
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.callbacks import BaseCallback
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False
    print("âš ï¸ Stable Baselines3 æœªå®‰è£…")

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from visual_training_monitor import VisualTrainingEnv
from metadrive.a_scen_env.trash.action_smoother import AdaptiveActionSmootherWrapper
from stable_ppo_config import get_stable_ppo_config, get_action_smoothing_config
from metadrive.a_scen_env.trash.episode_manager import EpisodeManager


class FixedTrainingCallback(BaseCallback):
    """
    ä¿®å¤ç‰ˆè®­ç»ƒå›è°ƒ - ç›‘æ§æ‰€æœ‰å¯èƒ½çš„é—®é¢˜
    """
    
    def __init__(self, log_freq=500, verbose=1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.episode_count = 0
        self.last_episode_count = 0
        self.stuck_detection_threshold = 100  # è¿ç»­100æ­¥æ²¡æœ‰episodeå˜åŒ–
        self.stuck_counter = 0
        self.last_reset_time = time.time()
        
    def _on_step(self) -> bool:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°episode
        if hasattr(self.training_env, 'episode_manager'):
            current_episodes = self.training_env.episode_manager.total_episodes
            if current_episodes > self.last_episode_count:
                self.stuck_counter = 0
                self.last_episode_count = current_episodes
                self.last_reset_time = time.time()
            else:
                self.stuck_counter += 1
        
        # å®šæœŸè¾“å‡ºçŠ¶æ€
        if self.num_timesteps % self.log_freq == 0:
            self._log_status()
        
        # æ£€æŸ¥æ˜¯å¦å¡æ­»
        if self.stuck_counter > self.stuck_detection_threshold:
            print(f"ğŸš¨ æ£€æµ‹åˆ°è®­ç»ƒå¡æ­»ï¼è¿ç»­{self.stuck_counter}æ­¥æ²¡æœ‰episodeå˜åŒ–")
            print(f"   å½“å‰step: {self.num_timesteps}")
            print(f"   ä¸Šæ¬¡é‡ç½®æ—¶é—´: {time.time() - self.last_reset_time:.1f}ç§’å‰")
            
            # å°è¯•å¼ºåˆ¶é‡ç½®ç¯å¢ƒ
            if hasattr(self.training_env, 'reset'):
                print("   å°è¯•å¼ºåˆ¶é‡ç½®ç¯å¢ƒ...")
                try:
                    self.training_env.reset()
                    self.stuck_counter = 0
                    self.last_reset_time = time.time()
                    print("   âœ… ç¯å¢ƒé‡ç½®æˆåŠŸ")
                except Exception as e:
                    print(f"   âŒ ç¯å¢ƒé‡ç½®å¤±è´¥: {e}")
                    # å¦‚æœé‡ç½®å¤±è´¥ï¼Œå¯èƒ½éœ€è¦åœæ­¢è®­ç»ƒ
                    return False
        
        return True
    
    def _log_status(self):
        """è¾“å‡ºçŠ¶æ€ä¿¡æ¯"""
        print(f"\nğŸ” è®­ç»ƒçŠ¶æ€æ£€æŸ¥ (Step {self.num_timesteps}):")
        print(f"   Episodeæ•°é‡: {self.last_episode_count}")
        print(f"   å¡æ­»è®¡æ•°å™¨: {self.stuck_counter}/{self.stuck_detection_threshold}")
        print(f"   è·ç¦»ä¸Šæ¬¡é‡ç½®: {time.time() - self.last_reset_time:.1f}ç§’")
        
        # æ˜¾ç¤ºç¯å¢ƒç»Ÿè®¡
        if hasattr(self.training_env, 'episode_manager'):
            stats = self.training_env.episode_manager.get_statistics()
            print(f"   å¼ºåˆ¶é‡ç½®æ¬¡æ•°: {stats.get('forced_resets', 0)}")
            print(f"   è¶…æ—¶é‡ç½®æ¬¡æ•°: {stats.get('timeout_resets', 0)}")
            print(f"   åœæ»é‡ç½®æ¬¡æ•°: {stats.get('stale_resets', 0)}")


def create_fixed_env(scenario_type="random", num_scenarios=100, seed=None):
    """
    åˆ›å»ºä¿®å¤ç‰ˆè®­ç»ƒç¯å¢ƒ
    """
    
    def _make_env():
        # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
        env = VisualTrainingEnv(
            scenario_type=scenario_type,
            num_scenarios=num_scenarios,
            seed=seed,
            visual_config={
                "render_mode": "human",
                "window_size": (1200, 800),
                "show_sensors": True,
                "show_trajectory": True,
            },
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šEpisodeç®¡ç†é…ç½®
            max_episode_steps=800,       # å‡å°‘æœ€å¤§æ­¥æ•°ï¼Œæ›´é¢‘ç¹é‡ç½®
            max_episode_time=120.0,      # 2åˆ†é’Ÿè¶…æ—¶
            force_reset_threshold=1000,  # å¼ºåˆ¶é‡ç½®é˜ˆå€¼
            stale_detection_steps=20     # åœæ»æ£€æµ‹æ­¥æ•°
        )
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ åŠ¨ä½œå¹³æ»‘
        env = AdaptiveActionSmootherWrapper(
            env,
            initial_smoothing=0.9,   # åˆæœŸå¼ºå¹³æ»‘
            final_smoothing=0.3,     # åæœŸç²¾ç¡®æ§åˆ¶
            adaptation_steps=20000,  # å‰20kæ­¥è‡ªé€‚åº”
            max_change_rate=0.2      # é™åˆ¶åŠ¨ä½œå˜åŒ–ç‡
        )
        
        print("ğŸ› ï¸ ä¿®å¤ç‰ˆç¯å¢ƒå·²åˆ›å»º:")
        print("   âœ… Episodeç®¡ç†å™¨")
        print("   âœ… åŠ¨ä½œå¹³æ»‘å™¨")
        print("   âœ… æ¸²æŸ“æ–‡æœ¬ä¿®å¤")
        
        return env
    
    return _make_env


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¿®å¤ç‰ˆPPOè®­ç»ƒ")
    
    parser.add_argument("--total-timesteps", type=int, default=50000,
                        help="æ€»è®­ç»ƒæ­¥æ•° (default: 50000)")
    parser.add_argument("--scenario-type", type=str, default="random",
                        choices=["random", "curriculum", "safe"],
                        help="åœºæ™¯ç±»å‹ (default: random)")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­ (default: 42)")
    parser.add_argument("--log-dir", type=str, default="./logs/fixed_training",
                        help="æ—¥å¿—ç›®å½• (default: ./logs/fixed_training)")
    parser.add_argument("--log-freq", type=int, default=500,
                        help="æ—¥å¿—é¢‘ç‡ (default: 500)")
    
    args = parser.parse_args()
    
    if not SB3_AVAILABLE:
        print("âŒ éœ€è¦å®‰è£… Stable Baselines3")
        print("å®‰è£…å‘½ä»¤: pip install stable-baselines3")
        return
    
    print("ğŸ› ï¸ å¯åŠ¨ä¿®å¤ç‰ˆPPOè®­ç»ƒ")
    print("=" * 60)
    print(f"è®­ç»ƒæ­¥æ•°: {args.total_timesteps:,}")
    print(f"åœºæ™¯ç±»å‹: {args.scenario_type}")
    print(f"éšæœºç§å­: {args.seed}")
    print(f"æ—¥å¿—ç›®å½•: {args.log_dir}")
    print("=" * 60)
    print("ğŸ”§ å·²é›†æˆä¿®å¤:")
    print("   - Episodeç®¡ç†å™¨ï¼ˆé˜²æ­¢å¡æ­»ï¼‰")
    print("   - åŠ¨ä½œå¹³æ»‘å™¨ï¼ˆç¨³å®šæ§åˆ¶ï¼‰")
    print("   - æ¸²æŸ“æ–‡æœ¬ä¿®å¤ï¼ˆæ— dtypeé”™è¯¯ï¼‰")
    print("   - ç¨³å®šPPOé…ç½®ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰")
    print("   - å¼ºåŒ–ç›‘æ§å›è°ƒï¼ˆå®æ—¶æ£€æµ‹ï¼‰")
    print("=" * 60)
    
    try:
        # åˆ›å»ºç¯å¢ƒ
        print("ğŸ—ï¸ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
        env = create_fixed_env(
            scenario_type=args.scenario_type,
            num_scenarios=100,
            seed=args.seed
        )()
        
        # è®¾ç½®æ—¥å¿—
        os.makedirs(args.log_dir, exist_ok=True)
        logger = configure(args.log_dir, ["stdout", "csv", "tensorboard"])
        
        # è·å–ç¨³å®šçš„PPOé…ç½®
        print("âš™ï¸ é…ç½®PPOå‚æ•°...")
        ppo_config = get_stable_ppo_config("visual_training")
        ppo_config.update({
            "tensorboard_log": args.log_dir,
            "seed": args.seed,
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ›´ä¿å®ˆçš„é…ç½®
            "learning_rate": 5e-5,    # æ›´ä½çš„å­¦ä¹ ç‡
            "n_steps": 512,           # æ›´å°çš„æ­¥æ•°
            "batch_size": 32,         # æ›´å°çš„æ‰¹æ¬¡
            "clip_range": 0.05,       # æ›´å°çš„è£å‰ªèŒƒå›´
            "ent_coef": 0.0001,       # æ›´å°çš„ç†µç³»æ•°
        })
        
        # åˆ›å»ºPPOæ¨¡å‹
        print("ğŸ¤– åˆ›å»ºPPOæ¨¡å‹...")
        model = PPO("MlpPolicy", env, **ppo_config)
        model.set_logger(logger)
        
        # åˆ›å»ºä¿®å¤ç‰ˆå›è°ƒ
        callback = FixedTrainingCallback(
            log_freq=args.log_freq,
            verbose=1
        )
        
        print(f"\nğŸ¯ å¼€å§‹ä¿®å¤ç‰ˆè®­ç»ƒ...")
        print(f"ğŸ’¡ ç›‘æ§æç¤º:")
        print(f"   - æ¯{args.log_freq}æ­¥è¾“å‡ºçŠ¶æ€æ£€æŸ¥")
        print(f"   - è‡ªåŠ¨æ£€æµ‹å¡æ­»å¹¶å¼ºåˆ¶é‡ç½®")
        print(f"   - åŠ¨ä½œå¹³æ»‘é€æ­¥é€‚åº”")
        print(f"   - Episodeè‡ªåŠ¨è¶…æ—¶ç®¡ç†")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # å¼€å§‹è®­ç»ƒ
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callback,
            progress_bar=True
        )
        
        # è®­ç»ƒå®Œæˆ
        training_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"   è€—æ—¶: {training_time/60:.1f}åˆ†é’Ÿ")
        print(f"   å¹³å‡é€Ÿåº¦: {args.total_timesteps/training_time:.1f} steps/s")
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(args.log_dir, "fixed_ppo_model")
        model.save(model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        if hasattr(env, 'episode_manager'):
            stats = env.episode_manager.get_statistics()
            print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            print(f"   æ€»Episodeæ•°: {stats['total_episodes']}")
            print(f"   å¼ºåˆ¶é‡ç½®: {stats['forced_resets']}")
            print(f"   è¶…æ—¶é‡ç½®: {stats['timeout_resets']}")
            print(f"   åœæ»é‡ç½®: {stats['stale_resets']}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
        if 'model' in locals():
            interrupted_path = os.path.join(args.log_dir, "fixed_ppo_interrupted")
            model.save(interrupted_path)
            print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupted_path}")
            
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if 'env' in locals():
            env.close()
            print("ğŸ”’ ç¯å¢ƒå·²å…³é—­")


if __name__ == "__main__":
    main() 