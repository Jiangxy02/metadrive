#!/usr/bin/env python3
"""
ç¨³å®šå¯è§†åŒ–PPOè®­ç»ƒè„šæœ¬ - è§£å†³ä¸»è½¦æ§åˆ¶ä¸ç¨³å®šé—®é¢˜

ä¸»è¦æ”¹è¿›ï¼š
1. é›†æˆåŠ¨ä½œå¹³æ»‘åŒ…è£…å™¨ï¼Œè§£å†³æ§åˆ¶æŠ–åŠ¨
2. ä½¿ç”¨ä¼˜åŒ–çš„PPOå‚æ•°é…ç½®
3. å¢å¼ºçš„è®­ç»ƒç›‘æ§å’Œè°ƒè¯•ä¿¡æ¯
4. è‡ªé€‚åº”è®­ç»ƒç­–ç•¥

ä½¿ç”¨æ–¹æ³•ï¼š
    python stable_visual_training.py --scenario-type random --total-timesteps 100000
"""

import argparse
import sys
import os
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥ä¾èµ–
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.callbacks import BaseCallback
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False
    print("âš ï¸ Stable Baselines3 æœªå®‰è£…")

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from visual_training_monitor import VisualTrainingEnv, VisualTrainingCallback
from metadrive.a_scen_env.trash.action_smoother import ActionSmootherWrapper, AdaptiveActionSmootherWrapper
from stable_ppo_config import get_stable_ppo_config, get_action_smoothing_config
from metadrive.a_scen_env.trash.render_text_fixer import fix_render_text
from metadrive.a_scen_env.trash.episode_manager import add_episode_management


class StableTrainingCallback(BaseCallback):
    """
    ç¨³å®šè®­ç»ƒå›è°ƒ - ç›‘æ§è®­ç»ƒç¨³å®šæ€§
    """
    
    def __init__(self, log_freq=1000, action_log_freq=100, verbose=1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.action_log_freq = action_log_freq
        self.action_history = []
        self.reward_history = []
        
    def _on_step(self) -> bool:
        # è®°å½•åŠ¨ä½œå’Œå¥–åŠ±
        if len(self.locals.get('infos', [])) > 0:
            info = self.locals['infos'][0]
            
            # è®°å½•åŠ¨ä½œä¿¡æ¯
            if 'smoothed_action' in info:
                self.action_history.append({
                    'step': self.num_timesteps,
                    'raw_action': info.get('raw_action', [0, 0]),
                    'smoothed_action': info['smoothed_action'],
                    'action_change': info.get('action_change', 0),
                    'smoothing_factor': info.get('current_smoothing_factor', 0.8)
                })
            
            # è®°å½•å¥–åŠ±
            if 'rewards' in self.locals:
                self.reward_history.append({
                    'step': self.num_timesteps,
                    'reward': self.locals['rewards'][0]
                })
        
        # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        if self.num_timesteps % self.log_freq == 0:
            self._log_training_stats()
            
        if self.num_timesteps % self.action_log_freq == 0:
            self._log_action_stats()
            
        return True
    
    def _log_training_stats(self):
        """è¾“å‡ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if len(self.reward_history) > 100:
            recent_rewards = [r['reward'] for r in self.reward_history[-100:]]
            avg_reward = np.mean(recent_rewards)
            reward_std = np.std(recent_rewards)
            
            print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡ (Step {self.num_timesteps}):")
            print(f"   è¿‘100æ­¥å¹³å‡å¥–åŠ±: {avg_reward:.3f} Â± {reward_std:.3f}")
    
    def _log_action_stats(self):
        """è¾“å‡ºåŠ¨ä½œç»Ÿè®¡ä¿¡æ¯"""
        if len(self.action_history) > 10:
            recent_actions = self.action_history[-10:]
            
            # è®¡ç®—åŠ¨ä½œå˜åŒ–
            action_changes = [a['action_change'] for a in recent_actions]
            avg_change = np.mean(action_changes)
            
            # è®¡ç®—åŠ¨ä½œèŒƒå›´
            raw_actions = np.array([a['raw_action'] for a in recent_actions])
            smoothed_actions = np.array([a['smoothed_action'] for a in recent_actions])
            
            raw_std = np.std(raw_actions, axis=0)
            smooth_std = np.std(smoothed_actions, axis=0)
            
            # è·å–å½“å‰å¹³æ»‘å› å­
            current_smoothing = recent_actions[-1].get('smoothing_factor', 0.8)
            
            print(f"ğŸ® åŠ¨ä½œç»Ÿè®¡ (Step {self.num_timesteps}):")
            print(f"   å¹³å‡åŠ¨ä½œå˜åŒ–: {avg_change:.4f}")
            print(f"   åŸå§‹åŠ¨ä½œæ ‡å‡†å·®: [{raw_std[0]:.3f}, {raw_std[1]:.3f}]")
            print(f"   å¹³æ»‘åŠ¨ä½œæ ‡å‡†å·®: [{smooth_std[0]:.3f}, {smooth_std[1]:.3f}]")
            print(f"   å½“å‰å¹³æ»‘å› å­: {current_smoothing:.3f}")


def create_stable_visual_env(scenario_type="random", num_scenarios=100, seed=None, 
                           use_action_smoothing=True, smoothing_config=None):
    """
    åˆ›å»ºç¨³å®šçš„å¯è§†åŒ–è®­ç»ƒç¯å¢ƒ
    """
    
    def _make_env():
        # åˆ›å»ºåŸºç¡€ç¯å¢ƒï¼ˆå·²ç»åŒ…å«episodeç®¡ç†ï¼‰
        env = VisualTrainingEnv(
            scenario_type=scenario_type,
            num_scenarios=num_scenarios,
            seed=seed,
            visual_config={
                "render_mode": "human",
                "window_size": (1200, 800),
                "show_sensors": True,
                "show_trajectory": True,
            },
            # Episodeç®¡ç†é…ç½®
            max_episode_steps=1000,      # æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
            max_episode_time=180.0,      # æ¯ä¸ªepisodeæœ€å¤§æ—¶é—´ï¼ˆ3åˆ†é’Ÿï¼‰
            force_reset_threshold=1200,  # å¼ºåˆ¶é‡ç½®é˜ˆå€¼
            stale_detection_steps=30     # åœæ»æ£€æµ‹æ­¥æ•°
        )
        
        # æ·»åŠ åŠ¨ä½œå¹³æ»‘
        if use_action_smoothing:
            config = smoothing_config or get_action_smoothing_config()
            
            if config["use_adaptive"]:
                env = AdaptiveActionSmootherWrapper(
                    env,
                    initial_smoothing=config["initial_smoothing"],
                    final_smoothing=config["final_smoothing"],
                    adaptation_steps=config["adaptation_steps"],
                    max_change_rate=config["max_change_rate"]
                )
                print(f"ğŸ”§ è‡ªé€‚åº”åŠ¨ä½œå¹³æ»‘å·²å¯ç”¨")
            else:
                env = ActionSmootherWrapper(
                    env,
                    smoothing_factor=config["smoothing_factor"],
                    max_change_rate=config["max_change_rate"]
                )
                print(f"ğŸ”§ åŸºç¡€åŠ¨ä½œå¹³æ»‘å·²å¯ç”¨")
        
        return env
    
    return _make_env


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç¨³å®šå¯è§†åŒ–PPOè®­ç»ƒ")
    
    # åŸºç¡€é…ç½®
    parser.add_argument("--scenario-type", type=str, default="random",
                        choices=["random", "curriculum", "safe"],
                        help="åœºæ™¯ç±»å‹ (default: random)")
    parser.add_argument("--total-timesteps", type=int, default=100000,
                        help="æ€»è®­ç»ƒæ­¥æ•° (default: 100000)")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­ (default: 42)")
    
    # ç¨³å®šæ€§é…ç½®
    parser.add_argument("--use-action-smoothing", action="store_true", default=True,
                        help="å¯ç”¨åŠ¨ä½œå¹³æ»‘ (default: True)")
    parser.add_argument("--smoothing-factor", type=float, default=0.8,
                        help="åŠ¨ä½œå¹³æ»‘å› å­ (default: 0.8)")
    parser.add_argument("--max-change-rate", type=float, default=0.25,
                        help="æœ€å¤§åŠ¨ä½œå˜åŒ–ç‡ (default: 0.25)")
    parser.add_argument("--use-adaptive-smoothing", action="store_true", default=True,
                        help="ä½¿ç”¨è‡ªé€‚åº”å¹³æ»‘ (default: True)")
    
    # PPOé…ç½®
    parser.add_argument("--learning-rate", type=float, default=1e-4,
                        help="å­¦ä¹ ç‡ (default: 1e-4)")
    parser.add_argument("--n-steps", type=int, default=1024,
                        help="æ¯æ¬¡æ›´æ–°æ­¥æ•° (default: 1024)")
    parser.add_argument("--batch-size", type=int, default=64,
                        help="æ‰¹é‡å¤§å° (default: 64)")
    parser.add_argument("--clip-range", type=float, default=0.08,
                        help="PPOè£å‰ªèŒƒå›´ (default: 0.08)")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--log-dir", type=str, default="./logs/stable_ppo_visual",
                        help="æ—¥å¿—ç›®å½• (default: ./logs/stable_ppo_visual)")
    parser.add_argument("--log-freq", type=int, default=1000,
                        help="æ—¥å¿—é¢‘ç‡ (default: 1000)")
    
    args = parser.parse_args()
    
    if not SB3_AVAILABLE:
        print("âŒ éœ€è¦å®‰è£… Stable Baselines3")
        print("å®‰è£…å‘½ä»¤: pip install stable-baselines3")
        return
    
    print("ğŸš— å¯åŠ¨ç¨³å®šå¯è§†åŒ–PPOè®­ç»ƒ")
    print("=" * 60)
    print(f"åœºæ™¯ç±»å‹: {args.scenario_type}")
    print(f"è®­ç»ƒæ­¥æ•°: {args.total_timesteps:,}")
    print(f"åŠ¨ä½œå¹³æ»‘: {'å¼€å¯' if args.use_action_smoothing else 'å…³é—­'}")
    print(f"è‡ªé€‚åº”å¹³æ»‘: {'å¼€å¯' if args.use_adaptive_smoothing else 'å…³é—­'}")
    print(f"å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"è£å‰ªèŒƒå›´: {args.clip_range}")
    print("=" * 60)
    
    try:
        # é…ç½®åŠ¨ä½œå¹³æ»‘
        smoothing_config = {
            "smoothing_factor": args.smoothing_factor,
            "max_change_rate": args.max_change_rate,
            "use_adaptive": args.use_adaptive_smoothing,
            "initial_smoothing": 0.95,
            "final_smoothing": 0.4,
            "adaptation_steps": args.total_timesteps // 3,  # å‰1/3è®­ç»ƒæ—¶é—´è‡ªé€‚åº”
        }
        
        # åˆ›å»ºç¯å¢ƒ
        env = create_stable_visual_env(
            scenario_type=args.scenario_type,
            num_scenarios=100,
            seed=args.seed,
            use_action_smoothing=args.use_action_smoothing,
            smoothing_config=smoothing_config
        )()
        
        # è®¾ç½®æ—¥å¿—
        os.makedirs(args.log_dir, exist_ok=True)
        logger = configure(args.log_dir, ["stdout", "csv", "tensorboard"])
        
        # è·å–ç¨³å®šçš„PPOé…ç½®
        ppo_config = get_stable_ppo_config("visual_training")
        
        # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
        ppo_config.update({
            "learning_rate": args.learning_rate,
            "n_steps": args.n_steps,
            "batch_size": args.batch_size,
            "clip_range": args.clip_range,
            "tensorboard_log": args.log_dir,
            "seed": args.seed,
        })
        
        # åˆ›å»ºPPOæ¨¡å‹
        print(f"\nğŸ¤– åˆ›å»ºPPOæ¨¡å‹...")
        model = PPO("MlpPolicy", env, **ppo_config)
        model.set_logger(logger)
        
        # åˆ›å»ºå›è°ƒ
        stable_callback = StableTrainingCallback(
            log_freq=args.log_freq,
            action_log_freq=100,
            verbose=1
        )
        
        visual_callback = VisualTrainingCallback(
            log_freq=args.log_freq,
            plot_freq=2000,
            verbose=1,
            training_env=env
        )
        
        print(f"\nğŸ¯ å¼€å§‹ç¨³å®šè®­ç»ƒ...")
        print(f"ğŸ’¡ è§‚å¯Ÿæç¤º:")
        print(f"   - è®­ç»ƒåˆæœŸåŠ¨ä½œä¼šæ¯”è¾ƒå¹³æ»‘ï¼Œéšè®­ç»ƒè¿›è¡Œé€æ¸ç²¾ç¡®")
        print(f"   - æ³¨æ„è§‚å¯ŸåŠ¨ä½œå˜åŒ–ç»Ÿè®¡ï¼Œç¡®è®¤å¹³æ»‘æ•ˆæœ")
        print(f"   - MetaDriveçª—å£æ˜¾ç¤ºå®æ—¶é©¾é©¶ï¼ŒMatplotlibæ˜¾ç¤ºè®­ç»ƒæ›²çº¿")
        
        # å¼€å§‹è®­ç»ƒ
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=[stable_callback, visual_callback],
            progress_bar=True
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(args.log_dir, "stable_ppo_model")
        model.save(model_path)
        print(f"\nâœ… ç¨³å®šæ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # è¾“å‡ºè®­ç»ƒæ‘˜è¦
        print(f"\nğŸ“ˆ è®­ç»ƒå®Œæˆæ‘˜è¦:")
        if len(stable_callback.action_history) > 0:
            final_smoothing = stable_callback.action_history[-1].get('smoothing_factor', 0.8)
            print(f"   æœ€ç»ˆå¹³æ»‘å› å­: {final_smoothing:.3f}")
        
        if len(stable_callback.reward_history) > 100:
            final_rewards = [r['reward'] for r in stable_callback.reward_history[-100:]]
            print(f"   æœ€ç»ˆ100æ­¥å¹³å‡å¥–åŠ±: {np.mean(final_rewards):.3f}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
        # ä¿å­˜ä¸­æ–­çš„æ¨¡å‹
        if 'model' in locals():
            interrupted_path = os.path.join(args.log_dir, "stable_ppo_interrupted")
            model.save(interrupted_path)
            print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupted_path}")
            
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if 'env' in locals():
            env.close()
            print("ğŸ”’ ç¯å¢ƒå·²å…³é—­")


if __name__ == "__main__":
    main() 