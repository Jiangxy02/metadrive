import numpy as np
import gymnasium as gym
from gymnasium import spaces


class ActionSmootherWrapper(gym.Wrapper):
    """
    åŠ¨ä½œå¹³æ»‘åŒ…è£…å™¨ - è§£å†³PPOè®­ç»ƒä¸­çš„æ§åˆ¶æŠ–åŠ¨é—®é¢˜
    
    åŠŸèƒ½ï¼š
    1. ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘åŠ¨ä½œ
    2. é™åˆ¶åŠ¨ä½œå˜åŒ–ç‡
    3. æä¾›å¯é…ç½®çš„å¹³æ»‘å‚æ•°
    """
    
    def __init__(self, env, smoothing_factor=0.8, max_change_rate=0.3):
        """
        åˆå§‹åŒ–åŠ¨ä½œå¹³æ»‘åŒ…è£…å™¨
        
        Args:
            env: åŸå§‹ç¯å¢ƒ
            smoothing_factor: å¹³æ»‘å› å­ [0, 1]ï¼Œè¶Šå¤§è¶Šå¹³æ»‘ï¼Œ0.8æ˜¯æ¨èå€¼
            max_change_rate: æœ€å¤§åŠ¨ä½œå˜åŒ–ç‡ [0, 1]ï¼Œé™åˆ¶å•æ­¥æœ€å¤§å˜åŒ–
        """
        super().__init__(env)
        
        self.smoothing_factor = smoothing_factor
        self.max_change_rate = max_change_rate
        
        # åˆå§‹åŒ–å†å²åŠ¨ä½œ
        self.last_action = np.zeros(self.action_space.shape, dtype=np.float32)
        self.step_count = 0
        
        print(f"ğŸ”§ ActionSmootherWrapper å·²å¯ç”¨:")
        print(f"   å¹³æ»‘å› å­: {smoothing_factor}")
        print(f"   æœ€å¤§å˜åŒ–ç‡: {max_change_rate}")
    
    def reset(self, **kwargs):
        """é‡ç½®ç¯å¢ƒæ—¶æ¸…ç©ºå†å²åŠ¨ä½œ"""
        obs, info = self.env.reset(**kwargs)
        self.last_action = np.zeros(self.action_space.shape, dtype=np.float32)
        self.step_count = 0
        return obs, info
    
    def step(self, action):
        """åº”ç”¨åŠ¨ä½œå¹³æ»‘åæ‰§è¡Œæ­¥éª¤"""
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, -1.0, 1.0)
        
        # ç¬¬ä¸€æ­¥ç›´æ¥ä½¿ç”¨åŸå§‹åŠ¨ä½œ
        if self.step_count == 0:
            smoothed_action = action.copy()
        else:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘
            smoothed_action = (
                self.smoothing_factor * self.last_action + 
                (1 - self.smoothing_factor) * action
            )
            
            # é™åˆ¶å˜åŒ–ç‡
            action_diff = smoothed_action - self.last_action
            max_change = self.max_change_rate
            
            # å¯¹æ¯ä¸ªç»´åº¦é™åˆ¶å˜åŒ–é‡
            for i in range(len(action_diff)):
                if abs(action_diff[i]) > max_change:
                    action_diff[i] = np.sign(action_diff[i]) * max_change
            
            smoothed_action = self.last_action + action_diff
            
            # å†æ¬¡ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
            smoothed_action = np.clip(smoothed_action, -1.0, 1.0)
        
        # æ›´æ–°å†å²
        self.last_action = smoothed_action.copy()
        self.step_count += 1
        
        # æ‰§è¡Œå¹³æ»‘åçš„åŠ¨ä½œ
        obs, reward, terminated, truncated, info = self.env.step(smoothed_action)
        
        # åœ¨infoä¸­æ·»åŠ åŠ¨ä½œä¿¡æ¯
        info['raw_action'] = action
        info['smoothed_action'] = smoothed_action
        info['action_change'] = np.linalg.norm(smoothed_action - 
                                             (action if self.step_count == 1 else self.last_action))
        
        return obs, reward, terminated, truncated, info


class AdaptiveActionSmootherWrapper(gym.Wrapper):
    """
    è‡ªé€‚åº”åŠ¨ä½œå¹³æ»‘åŒ…è£…å™¨ - æ ¹æ®è®­ç»ƒè¿›åº¦è°ƒæ•´å¹³æ»‘å¼ºåº¦
    
    è®­ç»ƒåˆæœŸä½¿ç”¨å¼ºå¹³æ»‘ï¼ŒåæœŸé€æ¸å‡å¼±å¹³æ»‘ï¼Œè®©æ¨¡å‹å­¦ä¼šç²¾ç¡®æ§åˆ¶
    """
    
    def __init__(self, env, initial_smoothing=0.9, final_smoothing=0.3, 
                 adaptation_steps=50000, max_change_rate=0.2):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”åŠ¨ä½œå¹³æ»‘åŒ…è£…å™¨
        
        Args:
            env: åŸå§‹ç¯å¢ƒ
            initial_smoothing: åˆå§‹å¹³æ»‘å› å­
            final_smoothing: æœ€ç»ˆå¹³æ»‘å› å­
            adaptation_steps: è‡ªé€‚åº”æ­¥æ•°
            max_change_rate: æœ€å¤§åŠ¨ä½œå˜åŒ–ç‡
        """
        super().__init__(env)
        
        self.initial_smoothing = initial_smoothing
        self.final_smoothing = final_smoothing
        self.adaptation_steps = adaptation_steps
        self.max_change_rate = max_change_rate
        
        # çŠ¶æ€å˜é‡
        self.last_action = np.zeros(self.action_space.shape, dtype=np.float32)
        self.global_step = 0
        self.episode_step = 0
        
        print(f"ğŸ”§ AdaptiveActionSmootherWrapper å·²å¯ç”¨:")
        print(f"   åˆå§‹å¹³æ»‘å› å­: {initial_smoothing}")
        print(f"   æœ€ç»ˆå¹³æ»‘å› å­: {final_smoothing}")
        print(f"   è‡ªé€‚åº”æ­¥æ•°: {adaptation_steps}")
    
    def _get_current_smoothing_factor(self):
        """æ ¹æ®è®­ç»ƒè¿›åº¦è®¡ç®—å½“å‰å¹³æ»‘å› å­"""
        if self.global_step >= self.adaptation_steps:
            return self.final_smoothing
        
        # çº¿æ€§è¡°å‡
        progress = self.global_step / self.adaptation_steps
        return self.initial_smoothing - (self.initial_smoothing - self.final_smoothing) * progress
    
    def reset(self, **kwargs):
        """é‡ç½®ç¯å¢ƒ"""
        obs, info = self.env.reset(**kwargs)
        self.last_action = np.zeros(self.action_space.shape, dtype=np.float32)
        self.episode_step = 0
        return obs, info
    
    def step(self, action):
        """æ‰§è¡Œè‡ªé€‚åº”å¹³æ»‘çš„æ­¥éª¤"""
        # æ›´æ–°æ­¥æ•°
        self.global_step += 1
        self.episode_step += 1
        
        # è·å–å½“å‰å¹³æ»‘å› å­
        current_smoothing = self._get_current_smoothing_factor()
        
        # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        action = np.clip(action, -1.0, 1.0)
        
        # åº”ç”¨å¹³æ»‘
        if self.episode_step == 1:
            smoothed_action = action.copy()
        else:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            smoothed_action = (
                current_smoothing * self.last_action + 
                (1 - current_smoothing) * action
            )
            
            # é™åˆ¶å˜åŒ–ç‡
            action_diff = smoothed_action - self.last_action
            max_change = self.max_change_rate
            
            for i in range(len(action_diff)):
                if abs(action_diff[i]) > max_change:
                    action_diff[i] = np.sign(action_diff[i]) * max_change
            
            smoothed_action = self.last_action + action_diff
            smoothed_action = np.clip(smoothed_action, -1.0, 1.0)
        
        # æ›´æ–°å†å²
        self.last_action = smoothed_action.copy()
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = self.env.step(smoothed_action)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        info.update({
            'raw_action': action,
            'smoothed_action': smoothed_action,
            'current_smoothing_factor': current_smoothing,
            'global_step': self.global_step,
            'adaptation_progress': min(1.0, self.global_step / self.adaptation_steps)
        })
        
        return obs, reward, terminated, truncated, info 