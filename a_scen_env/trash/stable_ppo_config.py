"""
ç¨³å®šPPOè®­ç»ƒé…ç½® - è§£å†³ä¸»è½¦æ§åˆ¶ä¸ç¨³å®šé—®é¢˜

é’ˆå¯¹MetaDriveç¯å¢ƒçš„PPOè®­ç»ƒä¼˜åŒ–é…ç½®ï¼Œè§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
1. åŠ¨ä½œæŠ–åŠ¨å’Œæ§åˆ¶ä¸ç¨³å®š
2. è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±æ³¢åŠ¨
3. ç½‘ç»œæ”¶æ•›ç¼“æ…¢

ä½¿ç”¨æ–¹æ³•ï¼š
    from stable_ppo_config import get_stable_ppo_config
    config = get_stable_ppo_config()
    model = PPO("MlpPolicy", env, **config)
"""

import numpy as np
from typing import Dict, Any


def get_stable_ppo_config(env_type: str = "visual_training") -> Dict[str, Any]:
    """
    è·å–ç¨³å®šçš„PPOè®­ç»ƒé…ç½®
    
    Args:
        env_type: ç¯å¢ƒç±»å‹ ("visual_training", "scenario", "random")
        
    Returns:
        PPOé…ç½®å­—å…¸
    """
    
    # åŸºç¡€ç¨³å®šé…ç½®
    base_config = {
        # === å­¦ä¹ ç‡é…ç½® ===
        "learning_rate": 2e-4,  # é™ä½å­¦ä¹ ç‡ï¼Œæé«˜ç¨³å®šæ€§
        
        # === è®­ç»ƒæ­¥æ•°é…ç½® ===
        "n_steps": 2048,        # å¢åŠ æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°ï¼Œæ”¶é›†æ›´å¤šç»éªŒ
        "batch_size": 128,      # å¢åŠ æ‰¹é‡å¤§å°ï¼Œå‡å°‘æ¢¯åº¦å™ªå£°
        "n_epochs": 4,          # å‡å°‘epochæ•°ï¼Œé¿å…è¿‡æ‹Ÿåˆ
        
        # === æŠ˜æ‰£å’ŒGAE ===
        "gamma": 0.995,         # ç¨å¾®å¢åŠ æŠ˜æ‰£å› å­ï¼Œé‡è§†é•¿æœŸå¥–åŠ±
        "gae_lambda": 0.98,     # å¢åŠ GAE lambdaï¼Œå‡å°‘æ–¹å·®
        
        # === PPOè£å‰ª ===
        "clip_range": 0.1,      # å‡å°è£å‰ªèŒƒå›´ï¼Œæ›´ä¿å®ˆçš„ç­–ç•¥æ›´æ–°
        "clip_range_vf": 0.1,   # ä»·å€¼å‡½æ•°è£å‰ª
        
        # === æŸå¤±æƒé‡ ===
        "ent_coef": 0.005,      # å‡å°ç†µç³»æ•°ï¼Œå‡å°‘éšæœºæ€§
        "vf_coef": 0.5,         # ä»·å€¼å‡½æ•°æŸå¤±æƒé‡
        
        # === æ¢¯åº¦è£å‰ª ===
        "max_grad_norm": 0.5,   # é™åˆ¶æ¢¯åº¦èŒƒæ•°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        
        # === ç½‘ç»œé…ç½® ===
        "policy_kwargs": {
            "net_arch": [256, 256],           # å¢åŠ ç½‘ç»œå®¹é‡
            "activation_fn": "tanh",          # ä½¿ç”¨tanhæ¿€æ´»å‡½æ•°ï¼ˆæ›´å¹³æ»‘ï¼‰
            "ortho_init": True,               # æ­£äº¤åˆå§‹åŒ–
            "use_sde": False,                 # ä¸ä½¿ç”¨çŠ¶æ€ä¾èµ–æ¢ç´¢
            "log_std_init": -0.5,             # åˆå§‹åŠ¨ä½œæ ‡å‡†å·®
            "full_std": True,                 # ä½¿ç”¨å®Œæ•´æ ‡å‡†å·®
            "use_expln": False,               # ä¸ä½¿ç”¨æŒ‡æ•°çº¿æ€§æ¿€æ´»
            "squash_output": False,           # ä¸å‹ç¼©è¾“å‡º
        },
        
        # === å…¶ä»–é…ç½® ===
        "verbose": 1,
        "seed": 42,
        "device": "auto",
    }
    
    # æ ¹æ®ç¯å¢ƒç±»å‹è°ƒæ•´é…ç½®
    if env_type == "visual_training":
        # å¯è§†åŒ–è®­ç»ƒï¼šæ›´ä¿å®ˆçš„é…ç½®
        base_config.update({
            "learning_rate": 1e-4,
            "n_steps": 1024,
            "batch_size": 64,
            "clip_range": 0.08,     # æ›´å°çš„è£å‰ªèŒƒå›´
            "ent_coef": 0.001,      # æ›´å°çš„ç†µç³»æ•°
        })
        
    elif env_type == "scenario":
        # åœºæ™¯è®­ç»ƒï¼šå¹³è¡¡é…ç½®
        base_config.update({
            "learning_rate": 3e-4,
            "n_steps": 2048,
            "batch_size": 128,
        })
        
    elif env_type == "random":
        # éšæœºç¯å¢ƒï¼šæ ‡å‡†é…ç½®
        pass  # ä½¿ç”¨åŸºç¡€é…ç½®
    
    return base_config


def get_action_smoothing_config() -> Dict[str, Any]:
    """
    è·å–åŠ¨ä½œå¹³æ»‘é…ç½®
    
    Returns:
        åŠ¨ä½œå¹³æ»‘åŒ…è£…å™¨é…ç½®
    """
    return {
        # === åŸºç¡€å¹³æ»‘é…ç½® ===
        "smoothing_factor": 0.8,    # å¹³æ»‘å› å­
        "max_change_rate": 0.25,    # æœ€å¤§å˜åŒ–ç‡
        
        # === è‡ªé€‚åº”å¹³æ»‘é…ç½® ===
        "use_adaptive": True,       # æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”å¹³æ»‘
        "initial_smoothing": 0.95,  # åˆå§‹å¹³æ»‘å› å­
        "final_smoothing": 0.4,     # æœ€ç»ˆå¹³æ»‘å› å­
        "adaptation_steps": 30000,  # è‡ªé€‚åº”æ­¥æ•°
    }


def get_environment_config() -> Dict[str, Any]:
    """
    è·å–ç¯å¢ƒé…ç½®ï¼Œå‡å°‘æ§åˆ¶éš¾åº¦
    
    Returns:
        ç¯å¢ƒé…ç½®å­—å…¸
    """
    return {
        # === è½¦è¾†é…ç½® ===
        "vehicle_config": {
            "show_navi_mark": False,
            "show_dest_mark": False,
            "show_line_to_dest": False,
            "show_line_to_navi_mark": False,
            "show_navigation_arrow": False,
            
            # === ç‰©ç†å‚æ•° ===
            "mass": 1200,               # è½¦è¾†è´¨é‡
            "max_engine_force": 1000,   # æœ€å¤§å¼•æ“åŠ›
            "max_brake_force": 1000,    # æœ€å¤§åˆ¶åŠ¨åŠ›
            "max_steering": 0.3,        # æœ€å¤§è½¬å‘è§’åº¦
            "friction": 0.8,            # è½®èƒæ‘©æ“¦åŠ›
        },
        
        # === ç¯å¢ƒå‚æ•° ===
        "physics_world_step_size": 0.02,  # ç‰©ç†æ­¥é•¿ (50Hz)
        "decision_repeat": 1,             # å†³ç­–é‡å¤æ¬¡æ•°
        "render_mode": "human",
        "use_render": True,
        
        # === å¥–åŠ±é…ç½® ===
        "crash_penalty": -10.0,          # ç¢°æ’æƒ©ç½š
        "out_of_road_penalty": -5.0,     # å‡ºç•Œæƒ©ç½š
        "speed_reward": 0.1,             # é€Ÿåº¦å¥–åŠ±
        "lane_center_reward": 0.05,      # è½¦é“ä¸­å¿ƒå¥–åŠ±
    }


def create_stable_training_env(base_env_class, env_config=None, use_action_smoothing=True):
    """
    åˆ›å»ºç¨³å®šçš„è®­ç»ƒç¯å¢ƒ
    
    Args:
        base_env_class: åŸºç¡€ç¯å¢ƒç±»
        env_config: ç¯å¢ƒé…ç½®
        use_action_smoothing: æ˜¯å¦ä½¿ç”¨åŠ¨ä½œå¹³æ»‘
        
    Returns:
        é…ç½®å¥½çš„è®­ç»ƒç¯å¢ƒ
    """
    from metadrive.a_scen_env.trash.action_smoother import ActionSmootherWrapper, AdaptiveActionSmootherWrapper
    
    # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
    env_config = env_config or get_environment_config()
    env = base_env_class(env_config)
    
    # æ·»åŠ åŠ¨ä½œå¹³æ»‘
    if use_action_smoothing:
        smoothing_config = get_action_smoothing_config()
        
        if smoothing_config["use_adaptive"]:
            env = AdaptiveActionSmootherWrapper(
                env,
                initial_smoothing=smoothing_config["initial_smoothing"],
                final_smoothing=smoothing_config["final_smoothing"],
                adaptation_steps=smoothing_config["adaptation_steps"],
                max_change_rate=smoothing_config["max_change_rate"]
            )
        else:
            env = ActionSmootherWrapper(
                env,
                smoothing_factor=smoothing_config["smoothing_factor"],
                max_change_rate=smoothing_config["max_change_rate"]
            )
    
    print("ğŸš— ç¨³å®šè®­ç»ƒç¯å¢ƒå·²åˆ›å»º:")
    print(f"   åŠ¨ä½œå¹³æ»‘: {'å¼€å¯' if use_action_smoothing else 'å…³é—­'}")
    print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"   è§‚æµ‹ç©ºé—´: {env.observation_space}")
    
    return env


# === ä½¿ç”¨ç¤ºä¾‹ ===
if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¦‚ä½•ä½¿ç”¨ç¨³å®šé…ç½®
    print("ğŸ”§ ç¨³å®šPPOé…ç½®ç¤ºä¾‹:")
    
    # è·å–PPOé…ç½®
    ppo_config = get_stable_ppo_config("visual_training")
    print(f"PPOé…ç½®: {ppo_config}")
    
    # è·å–åŠ¨ä½œå¹³æ»‘é…ç½®
    smooth_config = get_action_smoothing_config()
    print(f"åŠ¨ä½œå¹³æ»‘é…ç½®: {smooth_config}")
    
    # è·å–ç¯å¢ƒé…ç½®
    env_config = get_environment_config()
    print(f"ç¯å¢ƒé…ç½®: {env_config}")
    
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("from stable_ppo_config import get_stable_ppo_config, create_stable_training_env")
    print("ppo_config = get_stable_ppo_config('visual_training')")
    print("env = create_stable_training_env(YourEnvClass)")
    print("model = PPO('MlpPolicy', env, **ppo_config)") 