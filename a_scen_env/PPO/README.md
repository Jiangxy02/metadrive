# PPOè®­ç»ƒç³»ç»Ÿä½¿ç”¨è¯´æ˜

## ğŸ“‹ ç³»ç»Ÿæ¦‚è¿°

æœ¬ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„PPOï¼ˆProximal Policy Optimizationï¼‰è®­ç»ƒæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒ`TrajectoryReplayEnv`ç¯å¢ƒä¸­çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“ã€‚è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ç›´æ¥åœ¨`trajectory_replay.py`ä¸­ä½œä¸ºPPOä¸“å®¶ç­–ç•¥ä½¿ç”¨ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# ç¡®ä¿å·²å®‰è£…stable-baselines3
pip install stable-baselines3[extra]
pip install gymnasium
```

### 2. è®­ç»ƒæ¨¡å‹

æœ€ç®€å•çš„æ–¹å¼æ˜¯ä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼š

```bash
cd /home/jxy/æ¡Œé¢/1_Project/20250705_computational_cognitive_modeling/computational_cognitive_modeling/metadrive/a_scen_env/PPO
python start_training.py
```

æˆ–è€…ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

```bash
python train_ppo.py
```

### 3. è¯„ä¼°æ¨¡å‹

è®­ç»ƒå®Œæˆåï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š

```bash
# è¯„ä¼°æœ€æ–°çš„æ¨¡å‹
python evaluate_ppo.py

# è¯„ä¼°æŒ‡å®šæ¨¡å‹
python evaluate_ppo.py --model path/to/model.zip

# è¯„ä¼°æ—¶ä¸æ¸²æŸ“ï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰
python evaluate_ppo.py --no-render --episodes 10
```

### 4. åœ¨trajectory_replay.pyä¸­ä½¿ç”¨

è®­ç»ƒå¥½çš„æ¨¡å‹ä¼šè‡ªåŠ¨è¢«`trajectory_replay.py`æ£€æµ‹å¹¶ä½¿ç”¨ã€‚åªéœ€æ­£å¸¸è¿è¡Œtrajectory_replay.pyï¼ŒæŒ‰`T`é”®å³å¯åˆ‡æ¢åˆ°PPOä¸“å®¶æ¨¡å¼ã€‚

```bash
cd ..
python trajectory_replay.py
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
PPO/
â”œâ”€â”€ README.md              # æœ¬æ–‡æ¡£
â”œâ”€â”€ train_ppo.py          # PPOè®­ç»ƒä¸»è„šæœ¬
â”œâ”€â”€ evaluate_ppo.py       # æ¨¡å‹è¯„ä¼°è„šæœ¬
â”œâ”€â”€ ppo_expert.py         # PPOä¸“å®¶ç­–ç•¥æ¥å£
â”œâ”€â”€ start_training.py     # äº¤äº’å¼è®­ç»ƒå¯åŠ¨è„šæœ¬
â””â”€â”€ models/               # è®­ç»ƒå¥½çš„æ¨¡å‹å­˜å‚¨ç›®å½•
    â””â”€â”€ ppo_trajectory_YYYYMMDD_HHMMSS/
        â”œâ”€â”€ final_model.zip           # æœ€ç»ˆæ¨¡å‹
        â”œâ”€â”€ best_model/               # æœ€ä½³æ¨¡å‹
        â”‚   â””â”€â”€ best_model.zip
        â”œâ”€â”€ logs/                     # è®­ç»ƒæ—¥å¿—
        â””â”€â”€ ppo_checkpoint_*.zip      # æ£€æŸ¥ç‚¹
```

## âš™ï¸ è®­ç»ƒé…ç½®

### ç¯å¢ƒå‚æ•°

- `horizon`: æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤ï¼š2000ï¼‰
- `end_on_crash`: ç¢°æ’æ—¶æ˜¯å¦ç»“æŸï¼ˆé»˜è®¤ï¼šTrueï¼‰
- `end_on_out_of_road`: å‡ºç•Œæ—¶æ˜¯å¦ç»“æŸï¼ˆé»˜è®¤ï¼šTrueï¼‰
- `map`: åœ°å›¾é…ç½®ï¼Œ"S"*30è¡¨ç¤º30æ®µç›´é“
- `background_vehicle_update_mode`: èƒŒæ™¯è½¦æ›´æ–°æ¨¡å¼ï¼ˆ"position"æˆ–"dynamics"ï¼‰

### PPOè¶…å‚æ•°

åœ¨`train_ppo.py`ä¸­å¯ä»¥è°ƒæ•´ï¼š

- `learning_rate`: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ï¼š3e-4ï¼‰
- `n_steps`: æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°ï¼ˆé»˜è®¤ï¼š2048ï¼‰
- `batch_size`: æ‰¹å¤§å°ï¼ˆé»˜è®¤ï¼š64ï¼‰
- `n_epochs`: æ¯æ¬¡æ›´æ–°çš„epochæ•°ï¼ˆé»˜è®¤ï¼š10ï¼‰
- `gamma`: æŠ˜æ‰£å› å­ï¼ˆé»˜è®¤ï¼š0.99ï¼‰
- `clip_range`: PPOè£å‰ªèŒƒå›´ï¼ˆé»˜è®¤ï¼š0.2ï¼‰

### è®­ç»ƒå‚æ•°

- `total_timesteps`: æ€»è®­ç»ƒæ­¥æ•°ï¼ˆé»˜è®¤ï¼š500000ï¼‰
- `n_envs`: å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆé»˜è®¤ï¼š4ï¼‰
- `save_freq`: æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ï¼ˆé»˜è®¤ï¼š50000æ­¥ï¼‰
- `eval_freq`: è¯„ä¼°é¢‘ç‡ï¼ˆé»˜è®¤ï¼š10000æ­¥ï¼‰

## ğŸ® ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

### åœ¨trajectory_replay.pyä¸­ä½¿ç”¨

ç³»ç»Ÿä¼šè‡ªåŠ¨åŠ è½½æœ€æ–°çš„è®­ç»ƒæ¨¡å‹ã€‚æ§åˆ¶æ¨¡å¼åˆ‡æ¢çƒ­é”®ï¼š

- **T**: åˆ‡æ¢PPOä¸“å®¶/æ‰‹åŠ¨æ§åˆ¶æ¨¡å¼
- **E**: å¼€å…³PPOä¸“å®¶æ¥ç®¡
- **R**: å¼€å…³è½¨è¿¹é‡æ”¾æ¨¡å¼
- **M**: å¼ºåˆ¶è¿›å…¥æ‰‹åŠ¨æ¨¡å¼
- **W/A/S/D**: æ‰‹åŠ¨æ§åˆ¶ï¼ˆå‰è¿›/å·¦è½¬/åé€€/å³è½¬ï¼‰

### ç¨‹åºåŒ–ä½¿ç”¨

```python
from PPO.ppo_expert import expert, set_expert_model

# ä½¿ç”¨æœ€æ–°æ¨¡å‹
action = expert(agent)

# ä½¿ç”¨æŒ‡å®šæ¨¡å‹
set_expert_model("path/to/model.zip")
action = expert(agent)
```

## ğŸ“Š æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

ä½¿ç”¨TensorBoardæŸ¥çœ‹è®­ç»ƒè¿›åº¦ï¼š

```bash
tensorboard --logdir models/ppo_trajectory_*/logs
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰è®­ç»ƒé…ç½®

```python
from train_ppo import train_ppo

custom_config = {
    "horizon": 3000,
    "map": "S" * 50,  # æ›´é•¿çš„åœ°å›¾
    "background_vehicle_update_mode": "dynamics",  # ä½¿ç”¨åŠ¨åŠ›å­¦æ¨¡å¼
}

model, model_dir = train_ppo(
    total_timesteps=1000000,
    n_envs=8,
    config=custom_config
)
```

### ç»§ç»­è®­ç»ƒå·²æœ‰æ¨¡å‹

```python
from stable_baselines3 import PPO
from train_ppo import TrajectoryReplayWrapper

# åŠ è½½å·²æœ‰æ¨¡å‹
model = PPO.load("models/ppo_trajectory_*/final_model.zip")

# åˆ›å»ºç¯å¢ƒ
env = TrajectoryReplayWrapper(config)

# ç»§ç»­è®­ç»ƒ
model.set_env(env)
model.learn(total_timesteps=500000)
```

## â“ å¸¸è§é—®é¢˜

### Q: è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜å ç”¨è¿‡é«˜ï¼Ÿ
A: å‡å°‘å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆn_envsï¼‰æˆ–å‡å°‘n_stepså‚æ•°ã€‚

### Q: æ¨¡å‹æ”¶æ•›é€Ÿåº¦æ…¢ï¼Ÿ
A: å¯ä»¥å°è¯•ï¼š
- å¢åŠ è®­ç»ƒæ­¥æ•°ï¼ˆtotal_timestepsï¼‰
- è°ƒæ•´å­¦ä¹ ç‡ï¼ˆlearning_rateï¼‰
- ä¿®æ”¹å¥–åŠ±å‡½æ•°ï¼ˆéœ€è¦ä¿®æ”¹ç¯å¢ƒä»£ç ï¼‰

### Q: å¦‚ä½•ä½¿ç”¨å¤šä¸ªCSVæ–‡ä»¶è®­ç»ƒï¼Ÿ
A: è®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨`a_scen_env`ç›®å½•ä¸‹æ‰€æœ‰çš„`scenario_vehicles_*.csv`æ–‡ä»¶ã€‚

### Q: è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨trajectory_replay.pyä¸­è¡¨ç°ä¸å¥½ï¼Ÿ
A: å¯èƒ½éœ€è¦ï¼š
- å¢åŠ è®­ç»ƒæ­¥æ•°
- ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨ç›¸åŒçš„ç¯å¢ƒé…ç½®
- æ£€æŸ¥è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´æ˜¯å¦ä¸€è‡´

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **GPUåŠ é€Ÿ**: å¦‚æœæœ‰GPUï¼Œstable-baselines3ä¼šè‡ªåŠ¨ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒã€‚
2. **è®­ç»ƒæ—¶é—´**: 50ä¸‡æ­¥çš„è®­ç»ƒåœ¨4ä¸ªå¹¶è¡Œç¯å¢ƒä¸‹å¤§çº¦éœ€è¦1-2å°æ—¶ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰ã€‚
3. **æ¨¡å‹é€‰æ‹©**: ç³»ç»Ÿä¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆbest_modelï¼‰å’Œæœ€ç»ˆæ¨¡å‹ï¼ˆfinal_modelï¼‰ï¼Œé€šå¸¸best_modelæ€§èƒ½æ›´å¥½ã€‚
4. **å†…å­˜ç®¡ç†**: å¹¶è¡Œç¯å¢ƒä¼šå ç”¨è¾ƒå¤šå†…å­˜ï¼Œå¦‚æœå†…å­˜ä¸è¶³ï¼Œå‡å°‘n_envså‚æ•°ã€‚

## ğŸ¤ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·æŸ¥çœ‹ç›¸å…³æ–‡æ¡£æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚

---

*æœ€åæ›´æ–°ï¼š2025-01-03* 