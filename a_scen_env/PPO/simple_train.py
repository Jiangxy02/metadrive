#!/usr/bin/env python3
"""
ç®€åŒ–çš„PPOè®­ç»ƒè„šæœ¬

è§£å†³å¤šè¿›ç¨‹å’Œç¯å¢ƒåˆå§‹åŒ–é—®é¢˜çš„ç®€åŒ–ç‰ˆæœ¬
"""

import os
import sys
import numpy as np
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv

from trajectory_replay import TrajectoryReplayEnv
from trajectory_loader import load_trajectory


class SimpleTrajectoryWrapper(gym.Env):
    """
    ç®€åŒ–çš„GymnasiumåŒ…è£…å™¨
    é¿å…å¤æ‚çš„ç¯å¢ƒåˆ›å»ºå’Œå¤šè¿›ç¨‹é—®é¢˜
    """
    
    def __init__(self, csv_path=None):
        super().__init__()
        
        # ä½¿ç”¨å›ºå®šçš„CSVæ–‡ä»¶
        if csv_path is None:
            csv_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            csv_path = os.path.join(csv_dir, "scenario_vehicles_9_0_1_2_3_4_5_6_7_8_test3_20250304_150943_row001_filtered.csv")
        
        print(f"Loading trajectory from: {os.path.basename(csv_path)}")
        
        # åŠ è½½è½¨è¿¹æ•°æ®
        traj_data = load_trajectory(
            csv_path=csv_path,
            normalize_position=False,
            max_duration=50,  # ç¼©çŸ­åˆ°50ç§’
            use_original_position=False,
            translate_to_origin=True,
            target_fps=20.0,  # é™ä½é¢‘ç‡
            use_original_timestamps=False  # ä½¿ç”¨é‡é‡‡æ ·
        )
        
        # ç¯å¢ƒé…ç½®
        env_config = {
            "use_render": False,
            "manual_control": False,
            "horizon": 1000,  # å‡å°‘æ­¥æ•°
            "end_on_crash": True,
            "end_on_out_of_road": True,
            "end_on_arrive_dest": False,
            "end_on_horizon": True,
            "background_vehicle_update_mode": "position",
            "enable_realtime": False,
            "target_fps": 20.0,
            "map": "S" * 20,  # ç¼©çŸ­åœ°å›¾
            "traffic_density": 0.0,
            "disable_ppo_expert": True,  # ç¦ç”¨PPOä¸“å®¶
        }
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = TrajectoryReplayEnv(traj_data, config=env_config)
        
        # è·å–è§‚å¯Ÿç©ºé—´
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
            
        # å®šä¹‰ç©ºé—´
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=obs.shape,
            dtype=np.float32
        )
        
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0]),
            high=np.array([1.0, 1.0]),
            dtype=np.float32
        )
        
        print(f"Environment created successfully!")
        print(f"Observation space: {self.observation_space.shape}")
        print(f"Action space: {self.action_space}")
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
        
        return obs, {}
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        obs, reward, done, info = self.env.step(action)
        
        # å¤„ç†è¿”å›æ ¼å¼
        if isinstance(obs, tuple):
            obs = obs[0]
        
        return obs, reward, done, False, info
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        self.env.close()


def simple_train():
    """ç®€åŒ–çš„è®­ç»ƒå‡½æ•°"""
    print("=" * 60)
    print("Simple PPO Training")
    print("=" * 60)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_dir = os.path.join(os.path.dirname(__file__), "models", f"simple_ppo_{timestamp}")
    os.makedirs(model_dir, exist_ok=True)
    
    print(f"Model will be saved to: {model_dir}")
    
    # åˆ›å»ºç¯å¢ƒ
    print("\nCreating environment...")
    env = SimpleTrajectoryWrapper()
    env = DummyVecEnv([lambda: env])
    
    # PPOå‚æ•°
    ppo_params = {
        "learning_rate": 3e-4,
        "n_steps": 1024,  # å‡å°‘æ­¥æ•°
        "batch_size": 64,
        "n_epochs": 4,  # å‡å°‘epochs
        "gamma": 0.99,
        "clip_range": 0.2,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "policy": "MlpPolicy",
        "verbose": 1,
    }
    
    print("\nCreating PPO model...")
    model = PPO(env=env, **ppo_params)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=model_dir,
        name_prefix="simple_ppo",
    )
    
    # å¼€å§‹è®­ç»ƒ
    total_timesteps = 20000  # è¾ƒå°‘çš„æ­¥æ•°ç”¨äºæµ‹è¯•
    print(f"\nStarting training for {total_timesteps} timesteps...")
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True,
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(model_dir, "final_model")
        model.save(final_model_path)
        
        print(f"\nâœ… Training completed successfully!")
        print(f"Model saved to: {final_model_path}")
        
        return model, model_dir
        
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None
    finally:
        env.close()


if __name__ == "__main__":
    model, model_dir = simple_train()
    
    if model is not None:
        print("\n" + "=" * 60)
        print("ğŸ‰ SUCCESS! Your PPO model is ready!")
        print("=" * 60)
        print(f"Model location: {model_dir}")
        print("\nNext steps:")
        print("1. Test the model: python evaluate_ppo.py")
        print("2. Use in trajectory_replay.py (automatic)")
    else:
        print("\n" + "=" * 60)
        print("âŒ Training failed. Please check the errors above.")
        print("=" * 60) 