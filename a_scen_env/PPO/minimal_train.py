#!/usr/bin/env python3
"""
æœ€å°åŒ–PPOè®­ç»ƒè„šæœ¬

å®Œå…¨é¿å…èƒŒæ™¯è½¦å’Œå¤æ‚è½¨è¿¹åŠŸèƒ½ï¼Œä¸“æ³¨äºéªŒè¯è®­ç»ƒæµç¨‹
"""

import os
import sys
import numpy as np
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv

from metadrive.envs import MetaDriveEnv


class MinimalTrainingWrapper(gym.Env):
    """
    æœ€å°åŒ–çš„GymnasiumåŒ…è£…å™¨ï¼Œä½¿ç”¨æ ‡å‡†MetaDriveEnv
    """
    
    def __init__(self):
        super().__init__()
        
        # ç¯å¢ƒé…ç½®
        env_config = {
            "use_render": False,
            "manual_control": False,
            "horizon": 1000,
            "map": "S" * 20,  # ç®€å•çš„ç›´é“
            "traffic_density": 0.0,  # æ— èƒŒæ™¯äº¤é€š
            "start_seed": 0,
        }
        
        print("Creating minimal MetaDrive environment...")
        self.env = MetaDriveEnv(config=env_config)
        
        # è·å–è§‚å¯Ÿç©ºé—´
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
            
        # å®šä¹‰ç©ºé—´
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=obs.shape,
            dtype=np.float32
        )
        
        self.action_space = spaces.Box(
            low=np.array([-1.0, -1.0]),
            high=np.array([1.0, 1.0]),
            dtype=np.float32
        )
        
        print(f"âœ… Environment created successfully!")
        print(f"Observation space: {self.observation_space.shape}")
        print(f"Action space: {self.action_space}")
    
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
        
        return obs, {}
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        result = self.env.step(action)
        
        # å¤„ç†ä¸åŒçš„è¿”å›å€¼æ•°é‡
        if len(result) == 4:
            obs, reward, done, info = result
            terminated = done
            truncated = False
        elif len(result) == 5:
            obs, reward, terminated, truncated, info = result
            done = terminated or truncated
        else:
            raise ValueError(f"Unexpected number of return values: {len(result)}")
        
        # å¤„ç†è¿”å›æ ¼å¼
        if isinstance(obs, tuple):
            obs = obs[0]
        
        return obs, reward, terminated, truncated, info
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        self.env.close()


def minimal_train():
    """æœ€å°åŒ–è®­ç»ƒå‡½æ•°"""
    print("=" * 60)
    print("Minimal PPO Training (No Background Vehicles)")
    print("=" * 60)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_dir = os.path.join(os.path.dirname(__file__), "models", f"minimal_ppo_{timestamp}")
    os.makedirs(model_dir, exist_ok=True)
    
    print(f"Model will be saved to: {model_dir}")
    
    # åˆ›å»ºç¯å¢ƒ
    print("\nCreating environment...")
    env = MinimalTrainingWrapper()
    env = DummyVecEnv([lambda: env])
    
    # PPOå‚æ•°ï¼ˆæ›´ä¿å®ˆçš„è®¾ç½®ï¼‰
    ppo_params = {
        "learning_rate": 3e-4,
        "n_steps": 512,   # æ›´å°çš„æ­¥æ•°
        "batch_size": 32,  # æ›´å°çš„æ‰¹æ¬¡
        "n_epochs": 3,     # æ›´å°‘çš„epochs
        "gamma": 0.99,
        "clip_range": 0.2,
        "ent_coef": 0.01,
        "vf_coef": 0.5,
        "policy": "MlpPolicy",
        "verbose": 1,
        "device": "cpu",  # å¼ºåˆ¶ä½¿ç”¨CPU
    }
    
    print("\nCreating PPO model...")
    model = PPO(env=env, **ppo_params)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=2000,  # æ›´é¢‘ç¹çš„ä¿å­˜
        save_path=model_dir,
        name_prefix="minimal_ppo",
    )
    
    # å¼€å§‹è®­ç»ƒ
    total_timesteps = 10000  # æ›´å°‘çš„æ­¥æ•°ç”¨äºå¿«é€ŸéªŒè¯
    print(f"\nStarting training for {total_timesteps} timesteps...")
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True,
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(model_dir, "final_model")
        model.save(final_model_path)
        
        print(f"\nâœ… Training completed successfully!")
        print(f"Model saved to: {final_model_path}")
        
        return model, model_dir
        
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        return None, None
    finally:
        env.close()


if __name__ == "__main__":
    model, model_dir = minimal_train()
    
    if model is not None:
        print("\n" + "=" * 60)
        print("ğŸ‰ SUCCESS! Your minimal PPO model is ready!")
        print("=" * 60)
        print(f"Model location: {model_dir}")
        print("\nThis proves the PPO training system works!")
        print("Now you can:")
        print("1. Use this model in trajectory_replay.py")
        print("2. Extend to more complex scenarios")
    else:
        print("\n" + "=" * 60)
        print("âŒ Training failed. Please check the errors above.")
        print("=" * 60) 