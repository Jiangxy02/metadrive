#!/usr/bin/env python3
"""
å¯è§†åŒ–PPOè®­ç»ƒç›‘æ§å™¨
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶æ˜¾ç¤ºåœºæ™¯å’Œä¸»è½¦æ§åˆ¶æƒ…å†µ
"""

import sys
import os
import time
import threading
import queue
from typing import Dict, Any, Optional, List
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from random_scenario_generator import RandomScenarioGenerator, MetaDriveRandomEnv
from metadrive.a_scen_env.trash.render_text_fixer import fix_render_text
from metadrive.a_scen_env.trash.episode_manager import EpisodeManagedEnv

# SB3å’Œå…¶ä»–å¯¼å…¥
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.callbacks import BaseCallback
    from stable_baselines3.common.logger import configure
    import gymnasium as gym
    SB3_AVAILABLE = True
except ImportError:
    print("âŒ Stable Baselines3 æœªå®‰è£…")
    SB3_AVAILABLE = False
    # åˆ›å»ºè™šæ‹ŸBaseCallbackç”¨äºç±»å®šä¹‰
    class BaseCallback:
        def __init__(self, verbose=0):
            self.verbose = verbose
            self.n_calls = 0
            self.logger = None
        def _on_step(self):
            return True

# å¯è§†åŒ–ç›¸å…³å¯¼å…¥
try:
    import matplotlib.pyplot as plt
    import matplotlib.animation as animation
    from matplotlib.patches import Rectangle, Circle
    from collections import deque
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    print("âŒ Matplotlib æœªå®‰è£…ï¼Œæ— æ³•æ˜¾ç¤ºè®­ç»ƒæ›²çº¿")
    MATPLOTLIB_AVAILABLE = False


class VisualTrainingEnv(EpisodeManagedEnv, MetaDriveRandomEnv):
    """
    å¯è§†åŒ–è®­ç»ƒç¯å¢ƒ
    åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ç”¨æ¸²æŸ“å’Œè®°å½•
    """
    
    def __init__(self, 
                 scenario_type: str = "random",
                 num_scenarios: int = 1000,
                 seed: Optional[int] = None,
                 visual_config: Optional[Dict[str, Any]] = None,
                 **env_kwargs):
        """
        åˆå§‹åŒ–å¯è§†åŒ–è®­ç»ƒç¯å¢ƒ
        
        Args:
            visual_config: å¯è§†åŒ–é…ç½®
                - render_mode: æ¸²æŸ“æ¨¡å¼ ("human", "rgb_array")
                - window_size: çª—å£å¤§å°
                - record_video: æ˜¯å¦å½•åˆ¶è§†é¢‘
                - video_length: è§†é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
        """
        
        # å¯è§†åŒ–é…ç½®
        self.visual_config = visual_config or {
            "render_mode": "human",
            "window_size": [1200, 800],
            "record_video": False,
            "video_length": 30,
            "show_sensors": True,
            "show_trajectory": True,
        }
        
        # å¼ºåˆ¶å¯ç”¨æ¸²æŸ“
        window_size = self.visual_config.get("window_size", [1200, 800])
        if isinstance(window_size, list):
            window_size = tuple(window_size)  # è½¬æ¢ä¸ºtupleæ ¼å¼
            
        env_kwargs.update({
            "use_render": True,
            "window_size": window_size,
            "manual_control": False,  # å…³é—­æ‰‹åŠ¨æ§åˆ¶ï¼Œä½¿ç”¨PPO
        })
        
        # æå–MetaDriveRandomEnvéœ€è¦çš„å‚æ•°
        generator = RandomScenarioGenerator(seed=seed)
        base_config = {
            "num_scenarios": num_scenarios,
            "start_seed": seed or 0,
        }
        base_config.update(env_kwargs)
        
        super().__init__(
            generator=generator,
            scenario_type=scenario_type,
            base_config=base_config
        )
        
        # è®­ç»ƒæ•°æ®è®°å½•
        self.episode_count = 0
        self.step_count = 0
        self.current_episode_reward = 0
        self.current_episode_length = 0
        
        # æ€§èƒ½ç»Ÿè®¡
        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.action_history = deque(maxlen=1000)
        self.speed_history = deque(maxlen=1000)
        
        # å¯è§†åŒ–çŠ¶æ€
        self.render_enabled = True
        self.last_action = np.array([0.0, 0.0])
        self.predicted_action = np.array([0.0, 0.0])
        
        # å¥–åŠ±å‡½æ•°é…ç½®å’Œè¿½è¸ªï¼ˆä¸MetaDriveRandomWrapperä¿æŒä¸€è‡´ï¼‰
        self.reward_config = self._get_reward_config()
        self._last_position = None
        self._total_distance = 0.0
        self._episode_start_pos = None
        
    def reset(self, **kwargs):
        """é‡ç½®ç¯å¢ƒå¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        
        # è®°å½•ä¸Šä¸€ä¸ªepisodeçš„ç»Ÿè®¡
        if self.episode_count > 0:
            self.episode_rewards.append(self.current_episode_reward)
            self.episode_lengths.append(self.current_episode_length)
        
        # é‡ç½®è®¡æ•°å™¨
        self.current_episode_reward = 0
        self.current_episode_length = 0
        self.episode_count += 1
        
        # è°ƒç”¨çˆ¶ç±»reset
        result = super().reset(**kwargs)
        
        # é‡ç½®è¿›åº¦è·Ÿè¸ªå˜é‡
        if hasattr(self, 'agent') and hasattr(self.agent, 'position'):
            self._episode_start_pos = np.array(self.agent.position[:2])
            self._last_position = self._episode_start_pos.copy()
        else:
            self._episode_start_pos = np.array([0.0, 0.0])
            self._last_position = self._episode_start_pos.copy()
        
        self._total_distance = 0.0
        
        # é¦–æ¬¡æ¸²æŸ“
        if self.render_enabled:
            self.render_training_info()
        
        return result
    
    def step(self, action):
        """æ‰§è¡Œæ­¥éª¤å¹¶è®°å½•ä¿¡æ¯"""
        
        # è®°å½•é¢„æµ‹åŠ¨ä½œ
        self.predicted_action = np.array(action)
        
        # æ‰§è¡Œæ­¥éª¤
        result = super().step(action)
        
        # è§£æç»“æœ
        if len(result) == 4:
            obs, original_reward, done, info = result
            terminated = truncated = done
        else:
            obs, original_reward, terminated, truncated, info = result
            done = terminated or truncated
        
        # è®¡ç®—æ”¹è¿›çš„å¥–åŠ±
        improved_reward = self._compute_improved_reward(obs, action, done, info, original_reward)
        
        # æ›´æ–°ç»Ÿè®¡
        self.current_episode_reward += improved_reward
        self.current_episode_length += 1
        self.step_count += 1
        
        # è®°å½•å†å²æ•°æ®
        self.action_history.append(action.copy())
        if hasattr(self, 'agent') and hasattr(self.agent, 'speed'):
            self.speed_history.append(self.agent.speed)
        else:
            self.speed_history.append(0.0)
        
        # æ¸²æŸ“è®­ç»ƒä¿¡æ¯
        if self.render_enabled:
            self.render_training_info()
        
        # è¿”å›ä¿®æ”¹åçš„ç»“æœï¼ˆä½¿ç”¨æ”¹è¿›çš„å¥–åŠ±ï¼‰
        if len(result) == 4:
            return obs, improved_reward, done, info
        else:
            return obs, improved_reward, terminated, truncated, info
    
    def render_training_info(self):
        """æ¸²æŸ“è®­ç»ƒä¿¡æ¯åˆ°ç¯å¢ƒçª—å£"""
        
        try:
            # åŸºç¡€ç¯å¢ƒæ¸²æŸ“
            training_text = self._get_training_text()
            # ğŸ”§ ä¿®å¤æ¸²æŸ“æ–‡æœ¬ï¼Œé¿å…dtypeé”™è¯¯
            safe_text = fix_render_text(training_text)
            # ç›´æ¥è°ƒç”¨çˆ¶ç±»çš„renderæ–¹æ³•
            super().render(text=safe_text)
            
        except Exception as e:
            print(f"æ¸²æŸ“é”™è¯¯: {e}")
            # å°è¯•æ— æ–‡æœ¬æ¸²æŸ“
            try:
                super().render(text={})
            except:
                super().render()
    
    def _get_training_text(self) -> Dict[str, str]:
        """è·å–è®­ç»ƒä¿¡æ¯æ–‡æœ¬"""
        
        # åŸºç¡€ä¿¡æ¯
        training_info = {
            "ğŸ¯ Training Info": "=" * 20,
            "Episode": f"{self.episode_count}",
            "Step": f"{self.step_count}",
            "Episode Reward": f"{self.current_episode_reward:.2f}",
            "Episode Length": f"{self.current_episode_length}",
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        if self.episode_rewards:
            mean_reward = np.mean(list(self.episode_rewards))
            mean_length = np.mean(list(self.episode_lengths))
            training_info.update({
                "Mean Reward (100)": f"{mean_reward:.2f}",
                "Mean Length (100)": f"{mean_length:.1f}",
            })
        
        # å½“å‰åŠ¨ä½œä¿¡æ¯
        training_info.update({
            "ğŸ® Action Info": "=" * 15,
            "Predicted Action": f"[{self.predicted_action[0]:.2f}, {self.predicted_action[1]:.2f}]",
            "Steering": f"{self.predicted_action[0]:.2f}",
            "Throttle": f"{self.predicted_action[1]:.2f}",
        })
        
        # è½¦è¾†çŠ¶æ€
        if hasattr(self, 'agent'):
            agent = self.agent
            training_info.update({
                "ğŸš— Vehicle Info": "=" * 15,
                "Speed": f"{getattr(agent, 'speed', 0):.1f} m/s",
                "Position": f"({getattr(agent, 'position', [0, 0])[0]:.1f}, {getattr(agent, 'position', [0, 0])[1]:.1f})",
                "Heading": f"{getattr(agent, 'heading_theta', 0):.2f} rad",
            })
        
        # åœºæ™¯ä¿¡æ¯
        if hasattr(self.env, 'config'):
            config = self.env.config
            training_info.update({
                "ğŸ—ºï¸ Scenario Info": "=" * 15,
                "Map Blocks": f"{config.get('map', 'N/A')}",
                "Traffic Density": f"{config.get('traffic_density', 0):.2f}",
                "Accident Prob": f"{config.get('accident_prob', 0):.2f}",
            })
        
        return training_info
    
    def toggle_render(self):
        """åˆ‡æ¢æ¸²æŸ“çŠ¶æ€"""
        self.render_enabled = not self.render_enabled
        print(f"æ¸²æŸ“çŠ¶æ€: {'å¼€å¯' if self.render_enabled else 'å…³é—­'}")
    
    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        return {
            "episode_count": self.episode_count,
            "step_count": self.step_count,
            "episode_rewards": list(self.episode_rewards),
            "episode_lengths": list(self.episode_lengths),
            "action_history": list(self.action_history),
            "speed_history": list(self.speed_history),
        }
    
    def _get_reward_config(self) -> Dict[str, Any]:
        """è·å–æ”¹è¿›çš„å¥–åŠ±é…ç½®"""
        return {
            # å‰è¿›å¥–åŠ± - æ ¸å¿ƒé©±åŠ¨åŠ›
            "forward_reward_weight": 5.0,      # å‰è¿›è·ç¦»å¥–åŠ±æƒé‡ï¼ˆæœ€é‡è¦ï¼‰
            "speed_reward_weight": 1.0,        # åˆç†é€Ÿåº¦å¥–åŠ±æƒé‡
            
            # æ–¹å‘æ€§å¥–åŠ±
            "heading_reward_weight": 2.0,      # æœå‘æ­£ç¡®æ–¹å‘å¥–åŠ±
            "lane_center_weight": 0.5,         # è½¦é“ä¸­å¿ƒä¿æŒå¥–åŠ±
            
            # æƒ©ç½šé¡¹
            "crash_penalty": -20.0,            # ç¢°æ’ä¸¥é‡æƒ©ç½š
            "out_road_penalty": -10.0,         # å‡ºè·¯æƒ©ç½š
            "backward_penalty": -2.0,          # å€’é€€æƒ©ç½š
            "stop_penalty": -0.5,              # åœè½¦æƒ©ç½š
            
            # å®Œæˆå¥–åŠ±
            "completion_bonus": 50.0,          # åˆ°è¾¾ç»ˆç‚¹å¤§å¥–åŠ±
            "distance_bonus_threshold": 100.0, # è·ç¦»å¥–åŠ±é˜ˆå€¼
            
            # æ—¶é—´æƒ©ç½šï¼ˆè½»å¾®ï¼‰
            "time_penalty": -0.02,             # å‡å°‘æ¯æ­¥æƒ©ç½š
        }
    
    def _compute_improved_reward(self, obs, action, done, info, original_reward) -> float:
        """
        è®¡ç®—æ”¹è¿›çš„å¥–åŠ±å‡½æ•° - ä¸MetaDriveRandomWrapperä¿æŒä¸€è‡´
        """
        reward = 0.0
        config = self.reward_config
        
        # æ—¶é—´æƒ©ç½šï¼ˆè½»å¾®ï¼‰
        reward += config["time_penalty"]
        
        try:
            if hasattr(self, 'agent'):
                agent = self.agent
                
                # 1. å‰è¿›è·ç¦»å¥–åŠ±ï¼ˆæ ¸å¿ƒé©±åŠ¨åŠ›ï¼‰
                if hasattr(agent, 'position') and self._last_position is not None:
                    current_pos = np.array(agent.position[:2])
                    
                    # è®¡ç®—ç›¸å¯¹äºèµ·ç‚¹çš„æ€»è·ç¦»
                    total_distance_from_start = np.linalg.norm(current_pos - self._episode_start_pos)
                    
                    # åˆ¤æ–­æ˜¯å¦åœ¨å‰è¿›ï¼ˆåŸºäºä¸èµ·ç‚¹çš„è·ç¦»å¢åŠ ï¼‰
                    distance_increase = total_distance_from_start - self._total_distance
                    
                    if distance_increase > 0:
                        # å‰è¿›å¥–åŠ± - çº¿æ€§å¢é•¿
                        forward_reward = distance_increase * config["forward_reward_weight"]
                        reward += forward_reward
                        self._total_distance = total_distance_from_start
                    elif distance_increase < -0.5:  # æ˜æ˜¾åé€€
                        # å€’é€€æƒ©ç½š
                        reward += config["backward_penalty"]
                    
                    # æ›´æ–°ä½ç½®
                    self._last_position = current_pos
                
                # 2. é€Ÿåº¦å¥–åŠ±ï¼ˆé¼“åŠ±ä¿æŒåˆç†çš„å‰è¿›é€Ÿåº¦ï¼‰
                if hasattr(agent, 'speed'):
                    speed = agent.speed
                    
                    if speed < 1.0:  # åœè½¦æƒ©ç½š
                        reward += config["stop_penalty"]
                    elif 5.0 <= speed <= 25.0:  # ç†æƒ³é€Ÿåº¦èŒƒå›´
                        speed_reward = min(speed / 25.0, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
                        reward += config["speed_reward_weight"] * speed_reward
                    elif speed > 35.0:  # è¿‡å¿«æƒ©ç½š
                        reward += config["speed_reward_weight"] * (-0.5)
                
                # 3. æ–¹å‘å¥–åŠ±ï¼ˆæœå‘é“è·¯å‰æ–¹ï¼‰
                if hasattr(agent, 'heading_theta'):
                    heading = agent.heading_theta
                    # å¥–åŠ±æœå‘æ­£ç¡®æ–¹å‘ï¼ˆ-Ï€/4 åˆ° Ï€/4ï¼‰
                    heading_penalty = abs(heading) / (np.pi / 2)  # å½’ä¸€åŒ–
                    if heading_penalty < 0.5:  # æ–¹å‘åå·®ä¸å¤§
                        reward += config["heading_reward_weight"] * (1 - heading_penalty)
                
        except Exception as e:
            # å¦‚æœè·å–ä¿¡æ¯å¤±è´¥ï¼Œä¸å½±å“åŸºç¡€æµç¨‹
            pass
        
        # 5. ç»“æŸçŠ¶æ€å¥–åŠ±/æƒ©ç½š
        if done:
            if info.get("crash", False) or info.get("crash_vehicle", False):
                reward += config["crash_penalty"]
            elif info.get("out_of_road", False):
                reward += config["out_road_penalty"]
            elif info.get("arrive_dest", False):
                # åˆ°è¾¾ç›®æ ‡çš„å¤§å¥–åŠ±
                completion_reward = config["completion_bonus"]
                # é¢å¤–è·ç¦»å¥–åŠ±
                if self._total_distance > config["distance_bonus_threshold"]:
                    completion_reward += self._total_distance * 0.1
                reward += completion_reward
        
        return reward


class VisualTrainingCallback(BaseCallback):
    """
    å¯è§†åŒ–è®­ç»ƒå›è°ƒ
    é›†æˆè®­ç»ƒç›‘æ§å’Œå¯è§†åŒ–
    """
    
    def __init__(self, 
                 log_freq: int = 1000,
                 plot_freq: int = 5000,
                 save_freq: int = 10000,
                 verbose: int = 1,
                 training_env=None):
        super().__init__(verbose)
        
        self.log_freq = log_freq
        self.plot_freq = plot_freq
        self.save_freq = save_freq
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.training_steps = []
        self.loss_history = []
        
        # è®­ç»ƒç¯å¢ƒå¼•ç”¨
        self._training_env = training_env
        
        # å®æ—¶ç»˜å›¾è®¾ç½®
        if MATPLOTLIB_AVAILABLE:
            self.setup_realtime_plots()
        
    def setup_realtime_plots(self):
        """è®¾ç½®å®æ—¶ç»˜å›¾"""
        try:
            plt.ion()  # äº¤äº’æ¨¡å¼
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
                plt.rcParams['axes.unicode_minus'] = False
            except:
                pass  # å¿½ç•¥å­—ä½“è®¾ç½®å¤±è´¥
            
            self.fig, self.axes = plt.subplots(2, 2, figsize=(12, 8))
            self.fig.suptitle("PPO Training Monitor", fontsize=16)  # ä½¿ç”¨è‹±æ–‡é¿å…å­—ä½“é—®é¢˜
            
            # å­å›¾æ ‡é¢˜ï¼ˆä½¿ç”¨è‹±æ–‡ï¼‰
            self.axes[0, 0].set_title("Episode Rewards")
            self.axes[0, 1].set_title("Episode Lengths")
            self.axes[1, 0].set_title("Action Distribution")
            self.axes[1, 1].set_title("Speed Distribution")
            
            self.fig.tight_layout()
            self.plot_initialized = True
            
        except Exception as e:
            print(f"ç»˜å›¾åˆå§‹åŒ–å¤±è´¥: {e}")
            self.plot_initialized = False
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        
        # æ”¶é›†è®­ç»ƒæ•°æ®
        if self._training_env:
            if hasattr(self._training_env, 'envs'):
                # å‘é‡åŒ–ç¯å¢ƒ
                for i, env in enumerate(self._training_env.envs):
                    if hasattr(env, 'get_training_stats'):
                        stats = env.get_training_stats()
                        self._update_stats(stats)
            else:
                # å•ä¸€ç¯å¢ƒ
                if hasattr(self._training_env, 'get_training_stats'):
                    stats = self._training_env.get_training_stats()
                    self._update_stats(stats)
        
        # å®šæœŸæ—¥å¿—
        if self.n_calls % self.log_freq == 0:
            self._log_training_progress()
        
        # å®šæœŸç»˜å›¾
        if self.plot_initialized and self.n_calls % self.plot_freq == 0:
            self._update_plots()
        
        # å®šæœŸä¿å­˜
        if self.n_calls % self.save_freq == 0:
            self._save_training_snapshot()
        
        return True
    
    def _update_stats(self, stats: Dict[str, Any]):
        """æ›´æ–°è®­ç»ƒç»Ÿè®¡"""
        if stats['episode_rewards']:
            self.episode_rewards.extend(stats['episode_rewards'])
        if stats['episode_lengths']:
            self.episode_lengths.extend(stats['episode_lengths'])
    
    def _log_training_progress(self):
        """è®°å½•è®­ç»ƒè¿›åº¦"""
        if self.episode_rewards:
            recent_rewards = self.episode_rewards[-10:]
            recent_lengths = self.episode_lengths[-10:]
            
            mean_reward = np.mean(recent_rewards)
            mean_length = np.mean(recent_lengths)
            
            if self.verbose >= 1:
                print(f"\nğŸ¯ Step {self.n_calls}:")
                print(f"  Recent 10 episodes - Reward: {mean_reward:.2f}, Length: {mean_length:.1f}")
                
                if len(self.episode_rewards) >= 100:
                    last_100_reward = np.mean(self.episode_rewards[-100:])
                    print(f"  Last 100 episodes - Mean Reward: {last_100_reward:.2f}")
            
            # è®°å½•åˆ°tensorboard
            self.logger.record("train/mean_episode_reward", mean_reward)
            self.logger.record("train/mean_episode_length", mean_length)
            self.logger.record("train/total_episodes", len(self.episode_rewards))
    
    def _update_plots(self):
        """æ›´æ–°å®æ—¶ç»˜å›¾"""
        if not self.plot_initialized:
            return
            
        try:
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in self.axes.flat:
                ax.clear()
            
            # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
            if self.episode_rewards:
                self.axes[0, 0].plot(self.episode_rewards, 'b-', alpha=0.7)
                self.axes[0, 0].set_title(f"Episodeå¥–åŠ± (æœ€æ–°: {self.episode_rewards[-1]:.2f})")
                self.axes[0, 0].grid(True)
            
            # ç»˜åˆ¶é•¿åº¦æ›²çº¿
            if self.episode_lengths:
                self.axes[0, 1].plot(self.episode_lengths, 'g-', alpha=0.7)
                self.axes[0, 1].set_title(f"Episodeé•¿åº¦ (æœ€æ–°: {self.episode_lengths[-1]})")
                self.axes[0, 1].grid(True)
            
            # ç»˜åˆ¶åŠ¨ä½œåˆ†å¸ƒï¼ˆå¦‚æœæœ‰ç¯å¢ƒè®¿é—®æƒé™ï¼‰
            if self._training_env and hasattr(self._training_env, 'get_training_stats'):
                stats = self._training_env.get_training_stats()
                if stats['action_history']:
                    actions = np.array(stats['action_history'])
                    self.axes[1, 0].hist2d(actions[:, 0], actions[:, 1], bins=20, alpha=0.7)
                    self.axes[1, 0].set_title("Action Distribution")
                    self.axes[1, 0].set_xlabel("Steering")
                    self.axes[1, 0].set_ylabel("Throttle")
            
            # ç»˜åˆ¶é€Ÿåº¦åˆ†å¸ƒ
            if self._training_env and hasattr(self._training_env, 'get_training_stats'):
                stats = self._training_env.get_training_stats()
                if stats['speed_history']:
                    self.axes[1, 1].hist(stats['speed_history'], bins=30, alpha=0.7, color='orange')
                    self.axes[1, 1].set_title("Speed Distribution")
                    self.axes[1, 1].set_xlabel("Speed (m/s)")
                    self.axes[1, 1].set_ylabel("Frequency")
            
            plt.tight_layout()
            plt.pause(0.01)  # çŸ­æš‚æš‚åœä»¥æ›´æ–°æ˜¾ç¤º
            
        except Exception as e:
            print(f"ç»˜å›¾æ›´æ–°å¤±è´¥: {e}")
    
    def _save_training_snapshot(self):
        """ä¿å­˜è®­ç»ƒå¿«ç…§"""
        try:
            snapshot = {
                "step": self.n_calls,
                "episode_rewards": self.episode_rewards,
                "episode_lengths": self.episode_lengths,
                "timestamp": time.time(),
            }
            
            # ä¿å­˜åˆ°æ—¥å¿—ç›®å½•
            if hasattr(self.logger, 'dir'):
                snapshot_path = os.path.join(self.logger.dir, f"training_snapshot_{self.n_calls}.json")
                import json
                with open(snapshot_path, 'w') as f:
                    json.dump(snapshot, f, indent=2)
                
                if self.verbose >= 1:
                    print(f"ğŸ’¾ è®­ç»ƒå¿«ç…§å·²ä¿å­˜: {snapshot_path}")
                    
        except Exception as e:
            print(f"ä¿å­˜å¿«ç…§å¤±è´¥: {e}")


def create_visual_training_environment(scenario_type: str = "random",
                                     num_scenarios: int = 100,
                                     seed: Optional[int] = None,
                                     visual_config: Optional[Dict[str, Any]] = None,
                                     **config_kwargs):
    """
    åˆ›å»ºå¯è§†åŒ–è®­ç»ƒç¯å¢ƒ
    
    Args:
        scenario_type: åœºæ™¯ç±»å‹
        num_scenarios: åœºæ™¯æ•°é‡  
        seed: éšæœºç§å­
        visual_config: å¯è§†åŒ–é…ç½®
        **config_kwargs: é¢å¤–é…ç½®
        
    Returns:
        å¯è§†åŒ–è®­ç»ƒç¯å¢ƒ
    """
    
    def _make_env():
        return VisualTrainingEnv(
            scenario_type=scenario_type,
            num_scenarios=num_scenarios,
            seed=seed,
            visual_config=visual_config,
            **config_kwargs
        )
    
    return _make_env


def train_ppo_with_visualization(args):
    """
    å¼€å§‹å¯è§†åŒ–PPOè®­ç»ƒ
    """
    
    if not SB3_AVAILABLE:
        raise ImportError("Stable Baselines3 not available")
    
    print(f"ğŸ¬ å¼€å§‹å¯è§†åŒ–PPOè®­ç»ƒ")
    print(f"  åœºæ™¯ç±»å‹: {args.scenario_type}")
    print(f"  å¯è§†åŒ–: å¼€å¯")
    print(f"  è®­ç»ƒæ­¥æ•°: {args.total_timesteps}")
    
    # å¯è§†åŒ–é…ç½®
    window_size = getattr(args, 'window_size', [1200, 800])
    if isinstance(window_size, list):
        window_size = tuple(window_size)
        
    visual_config = {
        "render_mode": "human",
        "window_size": window_size,
        "show_sensors": getattr(args, 'show_sensors', True),
        "show_trajectory": getattr(args, 'show_trajectory', True),
    }
    
    # åˆ›å»ºå¯è§†åŒ–ç¯å¢ƒ
    env_config = {
        "scenario_type": args.scenario_type,
        "num_scenarios": getattr(args, 'num_scenarios', 100),
        "seed": getattr(args, 'seed', None),
        "visual_config": visual_config,
    }
    
    # å•ç¯å¢ƒè®­ç»ƒï¼ˆå¯è§†åŒ–æ¨¡å¼ä¸‹ä¸ä½¿ç”¨å¹¶è¡Œï¼‰
    env = create_visual_training_environment(**env_config)()
    
    # è®¾ç½®æ—¥å¿—
    log_dir = getattr(args, 'log_dir', './logs/ppo_visual')
    os.makedirs(log_dir, exist_ok=True)
    new_logger = configure(log_dir, ["stdout", "csv", "tensorboard"])
    
    # åˆ›å»ºPPOæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=getattr(args, 'learning_rate', 3e-4),
        n_steps=getattr(args, 'n_steps', 1024),  # è¾ƒå°çš„æ­¥æ•°ï¼Œä¾¿äºè§‚å¯Ÿ
        batch_size=getattr(args, 'batch_size', 64),
        n_epochs=getattr(args, 'n_epochs', 10),
        gamma=getattr(args, 'gamma', 0.99),
        gae_lambda=getattr(args, 'gae_lambda', 0.95),
        clip_range=getattr(args, 'clip_range', 0.2),
        tensorboard_log=log_dir,
        device=getattr(args, 'device', 'auto'),
        seed=getattr(args, 'seed', None)
    )
    
    model.set_logger(new_logger)
    
    # åˆ›å»ºå¯è§†åŒ–å›è°ƒ
    callback = VisualTrainingCallback(
        log_freq=getattr(args, 'log_freq', 500),
        plot_freq=getattr(args, 'plot_freq', 2000),
        verbose=1,
        training_env=env
    )
    
    print(f"\nğŸ¯ å¼€å§‹å¯è§†åŒ–è®­ç»ƒ...")
    print(f"ğŸ’¡ æç¤º: è®­ç»ƒè¿‡ç¨‹ä¸­å¯ä»¥è§‚çœ‹MetaDriveçª—å£å’Œå®æ—¶è®­ç»ƒæ›²çº¿")
    
    # å¼€å§‹è®­ç»ƒ
    try:
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callback,
            progress_bar=True
        )
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(log_dir, "ppo_visual_model")
        model.save(model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
        model_path = os.path.join(log_dir, "ppo_visual_interrupted")
        model.save(model_path)
        print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    finally:
        env.close()
        if MATPLOTLIB_AVAILABLE:
            plt.ioff()  # å…³é—­äº¤äº’æ¨¡å¼
    
    return model


if __name__ == "__main__":
    print("ğŸ¬ å¯è§†åŒ–PPOè®­ç»ƒå¯åŠ¨å™¨")
    
    # ç®€å•é…ç½®ç±»
    class Args:
        def __init__(self):
            self.scenario_type = "random"
            self.total_timesteps = 50000
            self.learning_rate = 3e-4
            self.n_steps = 1024
            self.batch_size = 64
            self.seed = 42
            self.log_freq = 500
            self.plot_freq = 2000
            self.window_size = (1200, 800)
    
    args = Args()
    
    try:
        model = train_ppo_with_visualization(args)
        print("\nğŸ‰ å¯è§†åŒ–è®­ç»ƒå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·é€€å‡º")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 