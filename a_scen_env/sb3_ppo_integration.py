#!/usr/bin/env python3
"""
Stable Baselines3 PPOé›†æˆæ–¹æ¡ˆ
æä¾›TrajectoryReplayEnvä¸SB3çš„æ¥å£é›†æˆ
"""

import sys
import os
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, Any, Optional, Union, Tuple

# å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from trajectory_loader import load_trajectory
from trajectory_replay import TrajectoryReplayEnv

# SB3ç›¸å…³å¯¼å…¥
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
    from stable_baselines3.common.callbacks import BaseCallback
    from stable_baselines3.common.logger import configure
    SB3_AVAILABLE = True
except ImportError:
    print("âš ï¸ Stable Baselines3 not found. Install with: pip install stable-baselines3")
    SB3_AVAILABLE = False


class SB3TrajectoryReplayWrapper(gym.Wrapper):
    """
    Stable Baselines3å…¼å®¹çš„è½¨è¿¹é‡æ”¾ç¯å¢ƒåŒ…è£…å™¨
    
    åŠŸèƒ½ï¼š
    1. ç¡®ä¿ä¸SB3çš„gym.Envæ¥å£å®Œå…¨å…¼å®¹
    2. å¤„ç†observation_spaceå’Œaction_space
    3. ç»Ÿä¸€stepè¿”å›æ ¼å¼ï¼ˆ5å…ƒç»„ï¼šobs, reward, terminated, truncated, infoï¼‰
    4. æä¾›åˆç†çš„é»˜è®¤å¥–åŠ±è®¾è®¡
    """
    
    def __init__(self, 
                 csv_path: str,
                 config: Optional[Dict[str, Any]] = None,
                 reward_config: Optional[Dict[str, Any]] = None,
                 max_duration: float = 30.0,
                 use_original_timestamps: bool = True):
        """
        åˆå§‹åŒ–SB3å…¼å®¹çš„ç¯å¢ƒ
        
        Args:
            csv_path: CSVè½¨è¿¹æ–‡ä»¶è·¯å¾„
            config: TrajectoryReplayEnvé…ç½®
            reward_config: å¥–åŠ±å‡½æ•°é…ç½®
            max_duration: æœ€å¤§ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰
            use_original_timestamps: æ˜¯å¦ä½¿ç”¨åŸå§‹æ—¶é—´æˆ³
        """
        
        # åŠ è½½è½¨è¿¹æ•°æ®
        traj_data = load_trajectory(
            csv_path=csv_path,
            max_duration=max_duration,
            use_original_timestamps=use_original_timestamps
        )
        
        # é»˜è®¤é…ç½®
        default_config = {
            "use_render": False,  # SB3è®­ç»ƒé€šå¸¸ä¸éœ€è¦æ¸²æŸ“
            "manual_control": False,  # ç¦ç”¨æ‰‹åŠ¨æ§åˆ¶
            "enable_realtime": False,  # ç¦ç”¨å®æ—¶æ¨¡å¼ï¼ŒåŠ é€Ÿè®­ç»ƒ
            "background_vehicle_update_mode": "position",  # ä½¿ç”¨ä½ç½®æ›´æ–°æ¨¡å¼
            "end_on_crash": True,  # ç¢°æ’æ—¶ç»“æŸ
            "end_on_out_of_road": True,  # å‡ºè·¯æ—¶ç»“æŸ
            "end_on_arrive_dest": True,  # åˆ°è¾¾ç»ˆç‚¹æ—¶ç»“æŸ
            "end_on_horizon": True,  # è¶…æ—¶æ—¶ç»“æŸ
        }
        
        if config:
            default_config.update(config)
            
        # åˆ›å»ºåŸºç¡€ç¯å¢ƒ
        base_env = TrajectoryReplayEnv(traj_data, config=default_config)
        
        super().__init__(base_env)
        
        # å¥–åŠ±é…ç½®
        self.reward_config = reward_config or self._get_default_reward_config()
        
        # è®°å½•è½¨è¿¹ä¿¡æ¯
        self.main_trajectory = base_env.main_vehicle_trajectory
        self.csv_path = csv_path
        self.max_duration = max_duration
        
        # ç¡®ä¿action_spaceå’Œobservation_spaceå…¼å®¹SB3
        self._setup_spaces()
        
        print(f"ğŸ¯ SB3ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ:")
        print(f"  è§‚æµ‹ç©ºé—´: {self.observation_space}")
        print(f"  åŠ¨ä½œç©ºé—´: {self.action_space}")
        print(f"  ä¸»è½¦è½¨è¿¹ç‚¹æ•°: {len(self.main_trajectory) if self.main_trajectory else 0}")
        
    def _get_default_reward_config(self) -> Dict[str, Any]:
        """é»˜è®¤å¥–åŠ±é…ç½®"""
        return {
            "speed_reward_weight": 0.1,      # é€Ÿåº¦å¥–åŠ±æƒé‡
            "progress_reward_weight": 1.0,   # è¿›åº¦å¥–åŠ±æƒé‡
            "lane_keeping_weight": 0.5,      # è½¦é“ä¿æŒæƒé‡
            "crash_penalty": -10.0,          # ç¢°æ’æƒ©ç½š
            "out_road_penalty": -5.0,        # å‡ºè·¯æƒ©ç½š
            "timeout_penalty": -1.0,         # è¶…æ—¶æƒ©ç½š
            "completion_bonus": 10.0,        # å®Œæˆå¥–åŠ±
        }
    
    def _setup_spaces(self):
        """è®¾ç½®è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´ï¼Œç¡®ä¿SB3å…¼å®¹æ€§"""
        
        # ä»åŸºç¡€ç¯å¢ƒè·å–ç©ºé—´å®šä¹‰
        base_obs_space = self.env.observation_space
        base_action_space = self.env.action_space
        
        # ç¡®ä¿æ˜¯gymnasiumæ ¼å¼
        if hasattr(base_obs_space, 'shape'):
            self.observation_space = spaces.Box(
                low=-np.inf, 
                high=np.inf, 
                shape=base_obs_space.shape, 
                dtype=np.float32
            )
        else:
            # é»˜è®¤è§‚æµ‹ç©ºé—´ï¼ˆMetaDriveé€šå¸¸æ˜¯49ç»´ï¼‰
            self.observation_space = spaces.Box(
                low=-np.inf, 
                high=np.inf, 
                shape=(49,), 
                dtype=np.float32
            )
            
        if hasattr(base_action_space, 'shape'):
            self.action_space = spaces.Box(
                low=-1.0, 
                high=1.0, 
                shape=base_action_space.shape, 
                dtype=np.float32
            )
        else:
            # é»˜è®¤åŠ¨ä½œç©ºé—´ï¼ˆè½¬å‘ï¼Œæ²¹é—¨/åˆ¹è½¦ï¼‰
            self.action_space = spaces.Box(
                low=-1.0, 
                high=1.0, 
                shape=(2,), 
                dtype=np.float32
            )
    
    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        """é‡ç½®ç¯å¢ƒï¼Œè¿”å›SB3å…¼å®¹æ ¼å¼"""
        
        if seed is not None:
            np.random.seed(seed)
            
        obs = self.env.reset()
        
        # ç¡®ä¿obsæ˜¯numpyæ•°ç»„
        if not isinstance(obs, np.ndarray):
            obs = np.array(obs, dtype=np.float32)
            
        info = {
            "episode_step": 0,
            "simulation_time": 0.0,
            "csv_path": self.csv_path
        }
        
        return obs, info
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›SB3å…¼å®¹çš„5å…ƒç»„æ ¼å¼"""
        
        # ç¡®ä¿actionæ˜¯æ­£ç¡®æ ¼å¼
        if not isinstance(action, np.ndarray):
            action = np.array(action, dtype=np.float32)
            
        # è°ƒç”¨åŸºç¡€ç¯å¢ƒstep
        obs, base_reward, done, info = self.env.step(action)
        
        # ç¡®ä¿obsæ˜¯numpyæ•°ç»„
        if not isinstance(obs, np.ndarray):
            obs = np.array(obs, dtype=np.float32)
            
        # è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±
        reward = self._compute_reward(obs, action, done, info)
        
        # SB3éœ€è¦åˆ†ç¦»terminatedå’Œtruncated
        terminated = done and (
            info.get("crash_flag", False) or 
            info.get("out_of_road_flag", False) or
            info.get("arrive_dest_flag", False)
        )
        truncated = done and info.get("horizon_reached_flag", False)
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        info.update({
            "base_reward": base_reward,
            "custom_reward": reward,
            "terminated": terminated,
            "truncated": truncated
        })
        
        return obs, reward, terminated, truncated, info
    
    def _compute_reward(self, obs, action, done, info) -> float:
        """è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""
        
        reward = 0.0
        config = self.reward_config
        
        # åŸºç¡€é€Ÿåº¦å¥–åŠ±
        if hasattr(self.env.agent, 'speed'):
            speed = self.env.agent.speed
            # é¼“åŠ±ä¿æŒåˆç†é€Ÿåº¦ï¼ˆ15-25 m/sï¼‰
            target_speed = 20.0
            speed_reward = -abs(speed - target_speed) / target_speed
            reward += config["speed_reward_weight"] * speed_reward
        
        # è¿›åº¦å¥–åŠ±ï¼ˆåŸºäºå‰è¿›è·ç¦»ï¼‰
        if hasattr(self.env.agent, 'position'):
            # ç®€å•çš„å‰è¿›å¥–åŠ±
            progress_reward = 0.1  # æ¯æ­¥åŸºç¡€å¥–åŠ±
            reward += config["progress_reward_weight"] * progress_reward
        
        # ç»“æŸçŠ¶æ€å¥–åŠ±/æƒ©ç½š
        if done:
            if info.get("crash_flag", False):
                reward += config["crash_penalty"]
            elif info.get("out_of_road_flag", False):
                reward += config["out_road_penalty"]
            elif info.get("arrive_dest_flag", False):
                reward += config["completion_bonus"]
            elif info.get("horizon_reached_flag", False):
                reward += config["timeout_penalty"]
        
        return reward


class TrajectoryReplayCallback(BaseCallback):
    """
    è½¨è¿¹é‡æ”¾è®­ç»ƒçš„SB3å›è°ƒå‡½æ•°
    """
    
    def __init__(self, log_freq: int = 1000, verbose: int = 0):
        super().__init__(verbose)
        self.log_freq = log_freq
        
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        
        if self.n_calls % self.log_freq == 0:
            # è®°å½•è®­ç»ƒç»Ÿè®¡
            if len(self.model.ep_info_buffer) > 0:
                ep_info = self.model.ep_info_buffer[-1]
                if self.verbose >= 1:
                    print(f"Step {self.n_calls}: Episode reward: {ep_info.get('r', 0):.2f}")
                    
        return True
    
    def _on_rollout_end(self) -> None:
        """å›åˆç»“æŸæ—¶è°ƒç”¨"""
        pass


def create_sb3_env(csv_path: str, 
                   config: Optional[Dict[str, Any]] = None,
                   reward_config: Optional[Dict[str, Any]] = None,
                   max_duration: float = 30.0,
                   use_original_timestamps: bool = True):
    """
    åˆ›å»ºSB3å…¼å®¹çš„ç¯å¢ƒå®ä¾‹
    
    Args:
        csv_path: CSVè½¨è¿¹æ–‡ä»¶è·¯å¾„
        config: ç¯å¢ƒé…ç½®
        reward_config: å¥–åŠ±é…ç½®
        max_duration: æœ€å¤§ä»¿çœŸæ—¶é•¿
        use_original_timestamps: æ˜¯å¦ä½¿ç”¨åŸå§‹æ—¶é—´æˆ³
        
    Returns:
        SB3å…¼å®¹çš„ç¯å¢ƒå®ä¾‹
    """
    
    def _make_env():
        return SB3TrajectoryReplayWrapper(
            csv_path=csv_path,
            config=config,
            reward_config=reward_config,
            max_duration=max_duration,
            use_original_timestamps=use_original_timestamps
        )
    
    return _make_env


def train_sb3_ppo(csv_path: str,
                  total_timesteps: int = 100000,
                  config: Optional[Dict[str, Any]] = None,
                  model_save_path: str = "sb3_ppo_model",
                  log_dir: str = "./sb3_logs/"):
    """
    ä½¿ç”¨SB3è®­ç»ƒPPOæ¨¡å‹
    
    Args:
        csv_path: CSVè½¨è¿¹æ–‡ä»¶è·¯å¾„
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        config: ç¯å¢ƒé…ç½®
        model_save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        log_dir: æ—¥å¿—ç›®å½•
        
    Returns:
        è®­ç»ƒå¥½çš„PPOæ¨¡å‹
    """
    
    if not SB3_AVAILABLE:
        raise ImportError("Stable Baselines3 not available. Install with: pip install stable-baselines3")
    
    print(f"ğŸš€ å¼€å§‹SB3 PPOè®­ç»ƒ:")
    print(f"  CSVæ–‡ä»¶: {csv_path}")
    print(f"  è®­ç»ƒæ­¥æ•°: {total_timesteps}")
    print(f"  ä¿å­˜è·¯å¾„: {model_save_path}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_sb3_env(csv_path, config)()
    
    # è®¾ç½®æ—¥å¿—
    os.makedirs(log_dir, exist_ok=True)
    new_logger = configure(log_dir, ["stdout", "csv", "tensorboard"])
    
    # åˆ›å»ºPPOæ¨¡å‹
    model = PPO(
        "MlpPolicy",  # å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥
        env,
        verbose=1,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        tensorboard_log=log_dir
    )
    
    # è®¾ç½®æ—¥å¿—
    model.set_logger(new_logger)
    
    # åˆ›å»ºå›è°ƒ
    callback = TrajectoryReplayCallback(log_freq=1000, verbose=1)
    
    # è®­ç»ƒæ¨¡å‹
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        progress_bar=True
    )
    
    # ä¿å­˜æ¨¡å‹
    model.save(model_save_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}")
    
    return model


def load_and_test_sb3_model(model_path: str,
                            csv_path: str,
                            num_episodes: int = 5,
                            config: Optional[Dict[str, Any]] = None,
                            render: bool = True):
    """
    åŠ è½½å¹¶æµ‹è¯•SB3æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        csv_path: æµ‹è¯•ç”¨CSVæ–‡ä»¶è·¯å¾„
        num_episodes: æµ‹è¯•å›åˆæ•°
        config: ç¯å¢ƒé…ç½®
        render: æ˜¯å¦æ¸²æŸ“
        
    Returns:
        æµ‹è¯•ç»“æœç»Ÿè®¡
    """
    
    if not SB3_AVAILABLE:
        raise ImportError("Stable Baselines3 not available")
    
    print(f"ğŸ§ª æµ‹è¯•SB3æ¨¡å‹: {model_path}")
    
    # æµ‹è¯•é…ç½®
    test_config = config or {}
    if render:
        test_config["use_render"] = True
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_sb3_env(csv_path, test_config)()
    
    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path)
    
    # æµ‹è¯•ç»Ÿè®¡
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        obs, info = env.reset()
        episode_reward = 0
        episode_length = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            if terminated or truncated:
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        print(f"Episode {episode + 1}: Reward={episode_reward:.2f}, Length={episode_length}")
    
    # è®¡ç®—ç»Ÿè®¡
    stats = {
        "mean_reward": np.mean(episode_rewards),
        "std_reward": np.std(episode_rewards),
        "mean_length": np.mean(episode_lengths),
        "std_length": np.std(episode_lengths),
        "episodes": num_episodes
    }
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {stats['mean_reward']:.2f} Â± {stats['std_reward']:.2f}")
    print(f"  å¹³å‡é•¿åº¦: {stats['mean_length']:.1f} Â± {stats['std_length']:.1f}")
    
    env.close()
    return stats


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    
    # CSVæ–‡ä»¶è·¯å¾„
    csv_path = "/home/jxy/æ¡Œé¢/1_Project/20250705_computational_cognitive_modeling/computational_cognitive_modeling/metadrive/a_scen_env/scenario_vehicles_9_0_1_2_3_4_5_6_7_8_test3_20250304_150943_row001_filtered.csv"
    
    # æ£€æŸ¥SB3å¯ç”¨æ€§
    if not SB3_AVAILABLE:
        print("âŒ Stable Baselines3 æœªå®‰è£…")
        print("å®‰è£…å‘½ä»¤: pip install stable-baselines3")
        exit(1)
    
    print("ğŸ¯ SB3 PPOé›†æˆç¤ºä¾‹")
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    test_config = {
        "use_render": False,
        "enable_realtime": False,
        "max_duration": 10.0
    }
    
    env = create_sb3_env(csv_path, config=test_config)()
    
    print(f"ç¯å¢ƒæµ‹è¯•:")
    obs, info = env.reset()
    print(f"  åˆå§‹è§‚æµ‹å½¢çŠ¶: {obs.shape}")
    print(f"  è§‚æµ‹ç©ºé—´: {env.observation_space}")
    print(f"  åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # æµ‹è¯•ä¸€æ­¥
    action = env.action_space.sample()
    obs, reward, terminated, truncated, info = env.step(action)
    print(f"  æ­¥è¿›æµ‹è¯•: reward={reward:.3f}, terminated={terminated}, truncated={truncated}")
    
    env.close()
    
    print("\nâœ… SB3é›†æˆæµ‹è¯•å®Œæˆ!")
    print("\nğŸš€ è®­ç»ƒå‘½ä»¤ç¤ºä¾‹:")
    print("python sb3_ppo_integration.py")
    print("\nğŸ“ ç¼–ç¨‹æ¥å£:")
    print("model = train_sb3_ppo(csv_path, total_timesteps=50000)")
    print("stats = load_and_test_sb3_model('sb3_ppo_model', csv_path)") 