#!/usr/bin/env python3
"""
åŸºäºMetaDriveéšæœºåœºæ™¯çš„SB3 PPOè®­ç»ƒè„šæœ¬
"""

import sys
import os
import argparse
import numpy as np
from typing import Dict, Any, Optional

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from random_scenario_generator import RandomScenarioGenerator, create_random_training_environment
from sb3_ppo_integration import SB3TrajectoryReplayWrapper

# SB3ç›¸å…³å¯¼å…¥
try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
    from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.utils import set_random_seed
    import gymnasium as gym
    from gymnasium import spaces
    SB3_AVAILABLE = True
except ImportError:
    print("âŒ Stable Baselines3 æœªå®‰è£…ã€‚å®‰è£…å‘½ä»¤: pip install stable-baselines3")
    SB3_AVAILABLE = False


class MetaDriveRandomWrapper(gym.Wrapper):
    """
    å°†MetaDriveéšæœºç¯å¢ƒåŒ…è£…ä¸ºSB3å…¼å®¹æ ¼å¼
    """
    
    def __init__(self, 
                 scenario_type: str = "curriculum",
                 num_scenarios: int = 1000,
                 seed: Optional[int] = None,
                 reward_config: Optional[Dict[str, Any]] = None,
                 **env_kwargs):
        """
        åˆå§‹åŒ–åŒ…è£…å™¨
        
        Args:
            scenario_type: åœºæ™¯ç±»å‹
            num_scenarios: åœºæ™¯æ€»æ•°
            seed: éšæœºç§å­
            reward_config: å¥–åŠ±é…ç½®
            **env_kwargs: ç¯å¢ƒé¢å¤–å‚æ•°
        """
        
        # ä¿å­˜åˆ›å»ºå‚æ•°ï¼ˆç”¨äºé‡æ–°åˆ›å»ºç¯å¢ƒï¼‰
        self._scenario_type = scenario_type
        self._num_scenarios = num_scenarios
        self._seed = seed
        self._env_kwargs = env_kwargs
        
        # åˆ›å»ºéšæœºç¯å¢ƒ
        base_env = self._create_base_env()
        
        super().__init__(base_env)
        
        # å¥–åŠ±é…ç½®
        self.reward_config = reward_config or self._get_default_reward_config()
        
        # è¿›åº¦è·Ÿè¸ªå˜é‡
        self._last_position = None
        self._total_distance = 0.0
        self._episode_start_pos = None
        
        # è®¾ç½®æ ‡å‡†åŒ–çš„åŠ¨ä½œå’Œè§‚æµ‹ç©ºé—´
        self._setup_spaces()
    
    def _create_base_env(self):
        """åˆ›å»ºåŸºç¡€ç¯å¢ƒ"""
        return create_random_training_environment(
            scenario_type=self._scenario_type,
            num_scenarios=self._num_scenarios,
            seed=self._seed,
            **self._env_kwargs
        )
        
    def _get_default_reward_config(self) -> Dict[str, Any]:
        """é»˜è®¤å¥–åŠ±é…ç½® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¼“åŠ±è½¦è¾†å‘å‰ç§»åŠ¨"""
        return {
            # å‰è¿›å¥–åŠ± - æ ¸å¿ƒé©±åŠ¨åŠ›
            "forward_reward_weight": 5.0,      # å‰è¿›è·ç¦»å¥–åŠ±æƒé‡ï¼ˆæœ€é‡è¦ï¼‰
            "speed_reward_weight": 1.0,        # åˆç†é€Ÿåº¦å¥–åŠ±æƒé‡
            
            # æ–¹å‘æ€§å¥–åŠ±
            "heading_reward_weight": 2.0,      # æœå‘æ­£ç¡®æ–¹å‘å¥–åŠ±
            "lane_center_weight": 0.5,         # è½¦é“ä¸­å¿ƒä¿æŒå¥–åŠ±
            
            # æƒ©ç½šé¡¹
            "crash_penalty": -20.0,            # ç¢°æ’ä¸¥é‡æƒ©ç½š
            "out_road_penalty": -10.0,         # å‡ºè·¯æƒ©ç½š
            "backward_penalty": -2.0,          # å€’é€€æƒ©ç½š
            "stop_penalty": -0.5,              # åœè½¦æƒ©ç½š
            
            # å®Œæˆå¥–åŠ±
            "completion_bonus": 50.0,          # åˆ°è¾¾ç»ˆç‚¹å¤§å¥–åŠ±
            "distance_bonus_threshold": 100.0, # è·ç¦»å¥–åŠ±é˜ˆå€¼
            
            # æ—¶é—´æƒ©ç½šï¼ˆè½»å¾®ï¼‰
            "time_penalty": -0.02,             # å‡å°‘æ¯æ­¥æƒ©ç½š
        }
    
    def _setup_spaces(self):
        """è®¾ç½®æ ‡å‡†åŒ–çš„è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´"""
        
        # ä½¿ç”¨åŸºç¡€ç¯å¢ƒçš„å®é™…ç©ºé—´ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
        self.observation_space = self.env.observation_space
        self.action_space = self.env.action_space
        
        print(f"ğŸ”§ ç¯å¢ƒç©ºé—´ä¿¡æ¯:")
        print(f"  è§‚æµ‹ç©ºé—´: {self.observation_space}")
        print(f"  åŠ¨ä½œç©ºé—´: {self.action_space}")
    
    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        """é‡ç½®ç¯å¢ƒ"""
        
        if seed is not None:
            np.random.seed(seed)
        
        # è°ƒç”¨åŸºç¡€ç¯å¢ƒçš„reset
        # MetaDriveçš„resetä¸æ”¯æŒseedå’Œoptionså‚æ•°ï¼Œæ‰€ä»¥ç›´æ¥è°ƒç”¨
        result = self.env.reset()
        
        # å¤„ç†è¿”å›æ ¼å¼ - MetaDriveè¿”å›(obs, info)å…ƒç»„
        if isinstance(result, tuple):
            obs, info = result
        else:
            obs = result
            info = {}
        
        # ç¡®ä¿obsæ ¼å¼æ­£ç¡®
        obs = np.array(obs, dtype=np.float32)
        if obs.shape[0] != self.observation_space.shape[0]:
            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå¡«å……æˆ–æˆªæ–­
            target_dim = self.observation_space.shape[0]
            if len(obs) < target_dim:
                # å¡«å……0
                padded_obs = np.zeros(target_dim, dtype=np.float32)
                padded_obs[:len(obs)] = obs
                obs = padded_obs
            else:
                # æˆªæ–­
                obs = obs[:target_dim]
        
        # é‡ç½®è¿›åº¦è·Ÿè¸ªå˜é‡
        if hasattr(self.env, 'agent') and hasattr(self.env.agent, 'position'):
            self._episode_start_pos = np.array(self.env.agent.position[:2])  # åªå–x,yåæ ‡
            self._last_position = self._episode_start_pos.copy()
        else:
            self._episode_start_pos = np.array([0.0, 0.0])
            self._last_position = self._episode_start_pos.copy()
        
        self._total_distance = 0.0
        
        return obs, info
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        
        # ç¡®ä¿actionæ ¼å¼æ­£ç¡®
        action = np.array(action, dtype=np.float32)
        
        # è°ƒç”¨åŸºç¡€ç¯å¢ƒ
        obs, reward, done, info = self.env.step(action)
        
        # å¤„ç†è§‚æµ‹æ ¼å¼
        obs = np.array(obs, dtype=np.float32)
        if obs.shape[0] != self.observation_space.shape[0]:
            target_dim = self.observation_space.shape[0]
            if len(obs) < target_dim:
                padded_obs = np.zeros(target_dim, dtype=np.float32)
                padded_obs[:len(obs)] = obs
                obs = padded_obs
            else:
                obs = obs[:target_dim]
        
        # è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±
        custom_reward = self._compute_reward(obs, action, done, info)
        
        # SB3æ ¼å¼ï¼šåˆ†ç¦»terminatedå’Œtruncated
        terminated = done and not info.get("timeout", False)
        truncated = done and info.get("timeout", False)
        
        # æ›´æ–°info
        info.update({
            "original_reward": reward,
            "custom_reward": custom_reward,
            "terminated": terminated,
            "truncated": truncated
        })
        
        return obs, custom_reward, terminated, truncated, info
    
    def _compute_reward(self, obs, action, done, info) -> float:
        """
        è®¡ç®—æ”¹è¿›çš„è‡ªå®šä¹‰å¥–åŠ±å‡½æ•° - å¼ºè°ƒå‰è¿›å’Œåˆ°è¾¾ç›®æ ‡
        """
        
        reward = 0.0
        config = self.reward_config
        
        # æ—¶é—´æƒ©ç½šï¼ˆè½»å¾®ï¼‰
        reward += config["time_penalty"]
        
        try:
            if hasattr(self.env, 'agent'):
                agent = self.env.agent
                
                # 1. å‰è¿›è·ç¦»å¥–åŠ±ï¼ˆæ ¸å¿ƒé©±åŠ¨åŠ›ï¼‰
                if hasattr(agent, 'position') and self._last_position is not None:
                    current_pos = np.array(agent.position[:2])
                    
                    # è®¡ç®—è¿™ä¸€æ­¥çš„å‰è¿›è·ç¦»
                    step_distance = np.linalg.norm(current_pos - self._last_position)
                    
                    # è®¡ç®—ç›¸å¯¹äºèµ·ç‚¹çš„æ€»è·ç¦»
                    total_distance_from_start = np.linalg.norm(current_pos - self._episode_start_pos)
                    
                    # åˆ¤æ–­æ˜¯å¦åœ¨å‰è¿›ï¼ˆåŸºäºä¸èµ·ç‚¹çš„è·ç¦»å¢åŠ ï¼‰
                    distance_increase = total_distance_from_start - self._total_distance
                    
                    if distance_increase > 0:
                        # å‰è¿›å¥–åŠ± - çº¿æ€§å¢é•¿
                        forward_reward = distance_increase * config["forward_reward_weight"]
                        reward += forward_reward
                        self._total_distance = total_distance_from_start
                    elif distance_increase < -0.5:  # æ˜æ˜¾åé€€
                        # å€’é€€æƒ©ç½š
                        reward += config["backward_penalty"]
                    
                    # æ›´æ–°ä½ç½®
                    self._last_position = current_pos
                
                # 2. é€Ÿåº¦å¥–åŠ±ï¼ˆé¼“åŠ±ä¿æŒåˆç†çš„å‰è¿›é€Ÿåº¦ï¼‰
                if hasattr(agent, 'speed'):
                    speed = agent.speed
                    
                    if speed < 1.0:  # åœè½¦æƒ©ç½š
                        reward += config["stop_penalty"]
                    elif 5.0 <= speed <= 25.0:  # ç†æƒ³é€Ÿåº¦èŒƒå›´
                        speed_reward = min(speed / 25.0, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
                        reward += config["speed_reward_weight"] * speed_reward
                    elif speed > 35.0:  # è¿‡å¿«æƒ©ç½š
                        reward += config["speed_reward_weight"] * (-0.5)
                
                # 3. æ–¹å‘å¥–åŠ±ï¼ˆæœå‘é“è·¯å‰æ–¹ï¼‰
                if hasattr(agent, 'heading_theta'):
                    # è¿™é‡Œå¯ä»¥æ ¹æ®é“è·¯æ–¹å‘è®¡ç®—ï¼Œæš‚æ—¶ç®€åŒ–
                    # å‡è®¾é“è·¯å¤§è‡´æ²¿xè½´æ­£æ–¹å‘
                    heading = agent.heading_theta
                    # å¥–åŠ±æœå‘æ­£ç¡®æ–¹å‘ï¼ˆ-Ï€/4 åˆ° Ï€/4ï¼‰
                    heading_penalty = abs(heading) / (np.pi / 2)  # å½’ä¸€åŒ–
                    if heading_penalty < 0.5:  # æ–¹å‘åå·®ä¸å¤§
                        reward += config["heading_reward_weight"] * (1 - heading_penalty)
                
                # 4. è½¦é“ä¸­å¿ƒå¥–åŠ±ï¼ˆå¦‚æœæœ‰ç›¸å…³ä¿¡æ¯ï¼‰
                # è¿™é‡Œéœ€è¦æ ¹æ®MetaDriveçš„è§‚æµ‹ä¿¡æ¯æ¥å®ç°
                # æš‚æ—¶ç®€åŒ–å¤„ç†
                
        except Exception as e:
            # å¦‚æœè·å–ä¿¡æ¯å¤±è´¥ï¼Œä¸å½±å“åŸºç¡€æµç¨‹
            pass
        
        # 5. ç»“æŸçŠ¶æ€å¥–åŠ±/æƒ©ç½š
        if done:
            if info.get("crash", False) or info.get("crash_vehicle", False):
                reward += config["crash_penalty"]
                print(f"ğŸš—ğŸ’¥ Crash penalty: {config['crash_penalty']}")
            elif info.get("out_of_road", False):
                reward += config["out_road_penalty"]
                print(f"ğŸ›£ï¸âŒ Out of road penalty: {config['out_road_penalty']}")
            elif info.get("arrive_dest", False):
                # åˆ°è¾¾ç›®æ ‡çš„å¤§å¥–åŠ±
                completion_reward = config["completion_bonus"]
                # é¢å¤–è·ç¦»å¥–åŠ±
                if self._total_distance > config["distance_bonus_threshold"]:
                    completion_reward += self._total_distance * 0.1
                reward += completion_reward
                print(f"ğŸ¯âœ… Completion bonus: {completion_reward}")
        
        return reward


class RandomScenarioCallback(BaseCallback):
    """
    éšæœºåœºæ™¯è®­ç»ƒå›è°ƒå‡½æ•°
    """
    
    def __init__(self, 
                 log_freq: int = 1000,
                 eval_freq: int = 10000,
                 verbose: int = 1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.eval_freq = eval_freq
        self.episode_rewards = []
        self.episode_lengths = []
        
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        
        # æ”¶é›†episodeç»Ÿè®¡
        if "episode" in self.locals["infos"][0]:
            ep_info = self.locals["infos"][0]["episode"]
            self.episode_rewards.append(ep_info["r"])
            self.episode_lengths.append(ep_info["l"])
        
        # å®šæœŸæ—¥å¿—
        if self.n_calls % self.log_freq == 0:
            if self.episode_rewards:
                mean_reward = np.mean(self.episode_rewards[-10:])  # æœ€è¿‘10ä¸ªepisode
                mean_length = np.mean(self.episode_lengths[-10:])
                
                if self.verbose >= 1:
                    print(f"Step {self.n_calls}: Mean reward: {mean_reward:.2f}, Mean length: {mean_length:.1f}")
                
                # è®°å½•åˆ°tensorboard
                self.logger.record("train/mean_episode_reward", mean_reward)
                self.logger.record("train/mean_episode_length", mean_length)
        
        return True


def create_training_environment(config: Dict[str, Any]):
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
    
    def _make_env():
        return MetaDriveRandomWrapper(**config)
    
    return _make_env


def train_ppo_random_scenarios(args):
    """è®­ç»ƒPPOæ¨¡å‹åœ¨éšæœºåœºæ™¯ä¸Š"""
    
    if not SB3_AVAILABLE:
        raise ImportError("Stable Baselines3 not available")
    
    print(f"ğŸš€ å¼€å§‹PPOéšæœºåœºæ™¯è®­ç»ƒ")
    print(f"  åœºæ™¯ç±»å‹: {args.scenario_type}")
    print(f"  åœºæ™¯æ•°é‡: {args.num_scenarios}")
    print(f"  è®­ç»ƒæ­¥æ•°: {args.total_timesteps}")
    print(f"  å¹¶è¡Œç¯å¢ƒ: {args.n_envs}")
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_random_seed(args.seed)
    
    # åˆ›å»ºç¯å¢ƒé…ç½®
    env_config = {
        "scenario_type": args.scenario_type,
        "num_scenarios": args.num_scenarios,
        "seed": args.seed,
    }
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    if args.n_envs > 1:
        env = make_vec_env(
            create_training_environment(env_config),
            n_envs=args.n_envs,
            seed=args.seed,
            vec_env_cls=DummyVecEnv  # æˆ–SubprocVecEnvç”¨äºçœŸæ­£çš„å¹¶è¡Œ
        )
    else:
        env = create_training_environment(env_config)()
    
    # è®¾ç½®æ—¥å¿—
    os.makedirs(args.log_dir, exist_ok=True)
    new_logger = configure(args.log_dir, ["stdout", "csv", "tensorboard"])
    
    # åˆ›å»ºPPOæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=args.learning_rate,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        n_epochs=args.n_epochs,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        clip_range=args.clip_range,
        ent_coef=args.ent_coef,
        vf_coef=args.vf_coef,
        max_grad_norm=args.max_grad_norm,
        tensorboard_log=args.log_dir,
        device=args.device,
        seed=args.seed
    )
    
    # è®¾ç½®æ—¥å¿—
    model.set_logger(new_logger)
    
    # åˆ›å»ºå›è°ƒ
    callback = RandomScenarioCallback(
        log_freq=args.log_freq,
        verbose=1
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    model.learn(
        total_timesteps=args.total_timesteps,
        callback=callback,
        progress_bar=True
    )
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(args.log_dir, "ppo_random_scenarios")
    model.save(model_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # æ¸…ç†ç¯å¢ƒ
    env.close()
    
    return model


def evaluate_model(model_path: str, args):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    print(f"\nğŸ§ª è¯„ä¼°æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_config = {
        "scenario_type": args.scenario_type,
        "num_scenarios": 20,  # è¯„ä¼°ç”¨å°‘é‡åœºæ™¯
        "seed": args.seed + 1000 if args.seed else None,  # ä¸åŒçš„ç§å­
    }
    
    env = create_training_environment(eval_config)()
    
    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path)
    
    # è¯„ä¼°ç»Ÿè®¡
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(args.eval_episodes):
        obs, info = env.reset()
        episode_reward = 0
        episode_length = 0
        
        while True:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_length += 1
            
            if terminated or truncated:
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸå®Œæˆ
                if info.get("arrive_dest", False):
                    success_count += 1
                break
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        if episode % 5 == 0:
            print(f"  Episode {episode + 1}: Reward={episode_reward:.2f}, Length={episode_length}")
    
    # è®¡ç®—ç»Ÿè®¡
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    mean_length = np.mean(episode_lengths)
    success_rate = success_count / args.eval_episodes
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
    print(f"  å¹³å‡é•¿åº¦: {mean_length:.1f}")
    print(f"  æˆåŠŸç‡: {success_rate:.2%}")
    
    env.close()


def main():
    parser = argparse.ArgumentParser(description="PPOéšæœºåœºæ™¯è®­ç»ƒ")
    
    # åœºæ™¯é…ç½®
    parser.add_argument("--scenario-type", type=str, default="random",
                        choices=["random", "curriculum", "safe"],
                        help="åœºæ™¯ç±»å‹")
    parser.add_argument("--num-scenarios", type=int, default=1000,
                        help="åœºæ™¯æ€»æ•°")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--total-timesteps", type=int, default=1000000,
                        help="æ€»è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--n-envs", type=int, default=4,
                        help="å¹¶è¡Œç¯å¢ƒæ•°")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­")
    
    # PPOå‚æ•°
    parser.add_argument("--learning-rate", type=float, default=3e-4,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--n-steps", type=int, default=2048,
                        help="æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°")
    parser.add_argument("--batch-size", type=int, default=64,
                        help="æ‰¹é‡å¤§å°")
    parser.add_argument("--n-epochs", type=int, default=10,
                        help="PPOæ›´æ–°è½®æ•°")
    parser.add_argument("--gamma", type=float, default=0.99,
                        help="æŠ˜æ‰£å› å­")
    parser.add_argument("--gae-lambda", type=float, default=0.95,
                        help="GAE lambda")
    parser.add_argument("--clip-range", type=float, default=0.2,
                        help="PPOè£å‰ªèŒƒå›´")
    parser.add_argument("--ent-coef", type=float, default=0.01,
                        help="ç†µç³»æ•°")
    parser.add_argument("--vf-coef", type=float, default=0.5,
                        help="ä»·å€¼å‡½æ•°ç³»æ•°")
    parser.add_argument("--max-grad-norm", type=float, default=0.5,
                        help="æ¢¯åº¦è£å‰ª")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--log-dir", type=str, default="./logs/ppo_random",
                        help="æ—¥å¿—ç›®å½•")
    parser.add_argument("--log-freq", type=int, default=1000,
                        help="æ—¥å¿—é¢‘ç‡")
    parser.add_argument("--device", type=str, default="auto",
                        help="è®¡ç®—è®¾å¤‡")
    
    # è¯„ä¼°é…ç½®
    parser.add_argument("--eval-episodes", type=int, default=20,
                        help="è¯„ä¼°å›åˆæ•°")
    parser.add_argument("--eval-only", action="store_true",
                        help="ä»…è¯„ä¼°æ¨¡å¼")
    parser.add_argument("--model-path", type=str,
                        help="è¯„ä¼°æ¨¡å‹è·¯å¾„")
    
    args = parser.parse_args()
    
    if args.eval_only:
        if not args.model_path:
            print("âŒ è¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š --model-path")
            return
        evaluate_model(args.model_path, args)
    else:
        # è®­ç»ƒæ¨¡å¼
        model = train_ppo_random_scenarios(args)
        
        # è®­ç»ƒåè¯„ä¼°
        model_path = os.path.join(args.log_dir, "ppo_random_scenarios")
        evaluate_model(model_path, args)


if __name__ == "__main__":
    main() 